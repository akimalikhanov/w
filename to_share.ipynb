{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869b8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import normaltest\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import lightgbm as lgb\n",
    "import sys\n",
    "import gc\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def return_size(df):\n",
    "    \"\"\"Return size of dataframe in gigabytes\"\"\"\n",
    "    return round(sys.getsizeof(df) / 1e9, 2)\n",
    "\n",
    "def convert_types(df, print_info = False):\n",
    "    \n",
    "    original_memory = df.memory_usage().sum()\n",
    "    \n",
    "    for c in df:\n",
    "        \n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "        \n",
    "        elif list(df[c].unique()) == [1, 0]:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        \n",
    "        elif df[c].dtype == np.float64:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        elif df[c].dtype == np.int64:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    new_memory = df.memory_usage().sum()\n",
    "    \n",
    "    if print_info:\n",
    "        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n",
    "        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def miss_table(data):\n",
    "    miss_table=data.isna().sum().to_frame(name='Count')\n",
    "    miss_table['Percent']=miss_table['Count']/len(data)*100\n",
    "    miss_table['Dtype']=data.dtypes[miss_table.index]\n",
    "    miss_table['Count']=miss_table['Count'].replace({0: np.nan})\n",
    "    miss_table=miss_table.dropna()\n",
    "    print(f\"There are {len(miss_table)}/{data.shape[1]} columns with missing values\")\n",
    "    print('Distribution by dtypes:')\n",
    "    print(miss_table['Dtype'].value_counts())\n",
    "    return miss_table.sort_values(by='Count', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "def generate_domain_features(df):\n",
    "    # CREDIT related \n",
    "    bins=[18, 35, 40, 50, 60, 70, 120]\n",
    "    labels=['18-29', '30-39', '40-49', '50-59', '60-69', '70+']\n",
    "    df['NEW_AGE_GROUP']=pd.cut(df['DAYS_BIRTH']/-365, bins=bins, labels=labels, right=False).astype('object')\n",
    "\n",
    "    cred_by_contract=df.groupby('NAME_CONTRACT_TYPE')['AMT_CREDIT'].mean() \n",
    "    cred_by_housing_type=df.groupby('NAME_HOUSING_TYPE')['AMT_CREDIT'].mean() \n",
    "    cred_by_org_type=df.groupby('ORGANIZATION_TYPE')['AMT_CREDIT'].mean() \n",
    "    cred_by_education_type=df.groupby('NAME_EDUCATION_TYPE')['AMT_CREDIT'].mean() \n",
    "    cred_by_gender=df.groupby('CODE_GENDER')['AMT_CREDIT'].mean() \n",
    "    cred_by_family_status=df.groupby('NAME_FAMILY_STATUS')['AMT_CREDIT'].mean()\n",
    "    cred_by_age_group=df.groupby('NEW_AGE_GROUP')['AMT_CREDIT'].mean()\n",
    "\n",
    "    df['NEW_AMT_CREDIT_TO_AMT_INCOME']=df['AMT_CREDIT']/df['AMT_INCOME_TOTAL'] \n",
    "    df['NEW_AMT_CREDIT_TO_AMT_ANNUITY']=df['AMT_CREDIT']/df['AMT_ANNUITY']\n",
    "    df['NEW_AMT_CREDIT_TO_AMT_GOODS_PRICE']=df['AMT_CREDIT']/df['AMT_GOODS_PRICE']\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_CREDIT_BY_CONTRACT_TYPE']=df['AMT_CREDIT']/(df['NAME_CONTRACT_TYPE'].map(cred_by_contract))\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_CREDIT_BY_HOUSING_TYPE']=df['AMT_CREDIT']/(df['NAME_HOUSING_TYPE'].map(cred_by_housing_type))\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_CREDIT_BY_ORGANIZATION_TYPE']=df['AMT_CREDIT']/(df['ORGANIZATION_TYPE'].map(cred_by_org_type))\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_CREDIT_BY_EDUCATION_TYPE']=df['AMT_CREDIT']/(df['NAME_EDUCATION_TYPE'].map(cred_by_education_type))\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_CREDIT_BY_GENDER']=df['AMT_CREDIT']/(df['CODE_GENDER'].map(cred_by_gender))\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_CREDIT_BY_FAMILY_STATUS']=df['AMT_CREDIT']/(df['NAME_FAMILY_STATUS'].map(cred_by_family_status))\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_INCOME_BY_AGE_GROUP']=df['AMT_CREDIT']/df['NEW_AGE_GROUP'].map(cred_by_age_group)\n",
    "\n",
    "\n",
    "    # INCOME related\n",
    "    inc_by_contract=df.groupby('NAME_CONTRACT_TYPE')['AMT_INCOME_TOTAL'].mean() \n",
    "    inc_by_housing_type=df.groupby('NAME_HOUSING_TYPE')['AMT_INCOME_TOTAL'].mean() \n",
    "    inc_by_org_type=df.groupby('ORGANIZATION_TYPE')['AMT_INCOME_TOTAL'].mean() \n",
    "    inc_by_education_type=df.groupby('NAME_EDUCATION_TYPE')['AMT_INCOME_TOTAL'].mean() \n",
    "    inc_by_gender=df.groupby('CODE_GENDER')['AMT_INCOME_TOTAL'].mean()\n",
    "    inc_by_family_status=df.groupby('NAME_FAMILY_STATUS')['AMT_INCOME_TOTAL'].mean()\n",
    "    inc_by_age_group=df.groupby('NEW_AGE_GROUP')['AMT_INCOME_TOTAL'].mean()\n",
    "\n",
    "    df['NEW_AMT_INCOME_BY_AGE_GROUP']=df['AMT_INCOME_TOTAL']/df['NEW_AGE_GROUP'].map(inc_by_age_group)\n",
    "    df['NEW_AMT_INCOME_BY_CNT_CHILD']=df['AMT_INCOME_TOTAL']/(1+df['CNT_CHILDREN'])\n",
    "    df['NEW_AMT_INCOME_BY_CNT_FAM_MEMBERS']=df['AMT_INCOME_TOTAL']/df['CNT_FAM_MEMBERS']\n",
    "    df['NEW_AMT_INCOME_BY_AGE']=df['AMT_INCOME_TOTAL']/(df['DAYS_BIRTH']/-365)\n",
    "    df['NEW_AMT_INCOME_TO_MEAN_AMT_CREDIT_BY_CONTRACT_TYPE']=df['AMT_INCOME_TOTAL']/(df['NAME_CONTRACT_TYPE'].map(inc_by_contract))\n",
    "    df['NEW_AMT_INCOME_TO_MEAN_AMT_CREDIT_BY_HOUSING_TYPE']=df['AMT_INCOME_TOTAL']/(df['NAME_HOUSING_TYPE'].map(inc_by_housing_type))\n",
    "    df['NEW_AMT_INCOME_TO_MEAN_AMT_CREDIT_BY_ORGANIZATION_TYPE']=df['AMT_INCOME_TOTAL']/(df['ORGANIZATION_TYPE'].map(inc_by_org_type))\n",
    "    df['NEW_AMT_INCOME_TO_MEAN_AMT_CREDIT_BY_EDUCATION_TYPE']=df['AMT_INCOME_TOTAL']/(df['NAME_EDUCATION_TYPE'].map(inc_by_education_type))\n",
    "    df['NEW_AMT_INCOME_TO_MEAN_AMT_CREDIT_BY_GENDER']=df['AMT_INCOME_TOTAL']/(df['CODE_GENDER'].map(inc_by_gender))\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_CREDIT_BY_FAMILY_STATUS']=df['AMT_CREDIT']/(df['NAME_FAMILY_STATUS'].map(inc_by_family_status))\n",
    "    df['NEW_AMT_INCOME_TO_MEAN_AMT_INCOME_BY_AGE_GROUP']=df['AMT_INCOME_TOTAL']/df['NEW_AGE_GROUP'].map(inc_by_age_group)\n",
    "\n",
    "\n",
    "    # FLAG related\n",
    "    # doc_flags--20 columns about documents\n",
    "    # contact_flags--6 flags about contact info of client (FLAG_MOBIL, FLAG_EMAIL, etc)\n",
    "    # address_flags--6 flags about address info of client (REG_REGION_NOT_LIVE_REGION, REG_REGION_NOT_WORK_REGION, etc)\n",
    "    doc_flags=[i for i in df.columns if 'FLAG_DOCUMENT' in i]\n",
    "    contact_flags=[i for i in df.columns if ('FLAG' in i) and (i not in doc_flags) and (i not in ('FLAG_OWN_CAR', 'FLAG_OWN_REALTY'))]\n",
    "    address_flags=[i for i in df.columns if 'NOT' in i]\n",
    "    flag_map={'Y':1, 'N':0}\n",
    "\n",
    "    df['NEW_DOC_FLAG_MEAN']=df[doc_flags].mean(axis=1)\n",
    "    df['NEW_DOC_FLAG_SUM']=df[doc_flags].sum(axis=1)\n",
    "    df['NEW_CONTACT_FLAG_MEAN']=df[contact_flags].mean(axis=1)\n",
    "    df['NEW_CONTACT_FLAG_SUM']=df[contact_flags].sum(axis=1)\n",
    "    df['NEW_ADDRESS_FLAG_MEAN']=df[address_flags].mean(axis=1)\n",
    "    df['NEW_ADDRESS_FLAG_SUM']=df[address_flags].sum(axis=1)\n",
    "    df['NEW_OWN_CAR_REALTY_COMBINATION']=0.75*df['FLAG_OWN_REALTY'].map(flag_map)+0.25*df['FLAG_OWN_CAR'].map(flag_map)\n",
    "\n",
    "\n",
    "    # AGE related\n",
    "    age_by_housing_type=df.groupby('NAME_HOUSING_TYPE')['DAYS_BIRTH'].mean()\n",
    "    age_by_own_realty=df.groupby('FLAG_OWN_REALTY')['DAYS_BIRTH'].mean()\n",
    "    age_by_own_car=df.groupby('FLAG_OWN_CAR')['DAYS_BIRTH'].mean()\n",
    "\n",
    "    df['NEW_AGE_TO_MEAN_AGE_BY_FLAG_OWN_REALTY']=df['DAYS_BIRTH']/(df['FLAG_OWN_REALTY'].map(age_by_own_realty))\n",
    "    df['NEW_AGE_TO_MEAN_AGE_BY_FLAG_OWN_CAR']=df['DAYS_BIRTH']/(df['FLAG_OWN_CAR'].map(age_by_own_car))\n",
    "    df['NEW_AGE_TO_MEAN_AGE_BY_HOUSING_TYPE']=df['DAYS_BIRTH']/(df['NAME_HOUSING_TYPE'].map(age_by_housing_type))\n",
    "    df[\"NEW_DAYS_EMPLOYED_TO_DAYS_BIRTH\"]=df['DAYS_EMPLOYED']/df['DAYS_BIRTH']\n",
    "    df[\"NEW_DAYS_REGISTRATION_TO_DAYS_BIRTH\"]=df['DAYS_REGISTRATION']/df['DAYS_BIRTH']\n",
    "\n",
    "\n",
    "    # Other\n",
    "    df['NEW_OWN_CAR_AGE_TO_DAYS_BIRTH']=df['OWN_CAR_AGE']/df['DAYS_BIRTH']\n",
    "    df['NEW_OWN_CAR_AGE_TO_DAYS_EMPLOYED']=df['OWN_CAR_AGE']/df['DAYS_EMPLOYED']\n",
    "    df['NEW_DAYS_LAST_PHONE_CHANGE_TO_DAYS_BIRTH']=df['DAYS_LAST_PHONE_CHANGE']/df['DAYS_BIRTH']\n",
    "    df['NEW_DAYS_LAST_PHONE_CHANGE_TO_DAYS_EMPLOYED']=df['DAYS_LAST_PHONE_CHANGE']/df['DAYS_EMPLOYED']\n",
    "    df['NEW_CNT_CHILD_TO_CNT_FAM_MEMBERS']=df['CNT_CHILDREN']/df['CNT_FAM_MEMBERS']\n",
    "    df['NEW_EXT_SOURCES_MEAN']=df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    df['NEW_EXT_SOURCES_STD']=df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "    df['NEW_DAYS_CHANGE_MEAN']=df[['DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE', 'DAYS_REGISTRATION']].mean(axis=1)\n",
    "    df['NEW_REGION_RATING_CLIENT_MEAN']=df[['REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY']].mean(axis=1)\n",
    "    df['NEW_30_CNT_SOCIAL_CIRCLE_MEAN']=df[['OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE']].mean(axis=1)\n",
    "    df['NEW_60_CNT_SOCIAL_CIRCLE_MEAN']=df[['OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE']].mean(axis=1)\n",
    "    \n",
    "    print(f'After adding features: {df.shape}')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def numeric_agg(df, group_col, df_name, remove_dup=True):\n",
    "    num_df=df.select_dtypes('number')\n",
    "    if num_df.shape[1]!=0:\n",
    "        for c in num_df.columns:\n",
    "            if 'ID' in c and c!=group_col:\n",
    "                num_df=num_df.drop(c, axis=1)\n",
    "        df_agg=num_df.groupby(group_col).agg(['count', 'mean', 'max', 'min', 'sum'])\n",
    "\n",
    "        new_cols=[]\n",
    "        for l1 in df_agg.columns.levels[0]:\n",
    "            if l1!=group_col:\n",
    "                for l2 in df_agg.columns.levels[1]: # for stat in agg.columns.levels[1][:-1]\n",
    "                    new_cols.append(f'{df_name}_{l1}_{l2}')\n",
    "        df_agg.columns=new_cols\n",
    "        if remove_dup:\n",
    "            # Remove duplicate columns by values\n",
    "            _, idx = np.unique(df_agg, axis = 1, return_index = True)\n",
    "            df_agg = df_agg.iloc[:, idx]\n",
    "        print(f'Dataset:{df_name}\\n\\tBefore: {num_df.shape[1]} numeric cols\\n\\tAfter: {df_agg.shape[1]}')\n",
    "        return df_agg\n",
    "    else:\n",
    "        print('No numeric columns in dataframe')\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "def categ_agg(df, group_col, df_name, enc, enc_mode='train', remove_dup=True):\n",
    "    cat_df=df.select_dtypes(include=['object'])\n",
    "    if cat_df.shape[1]!=0:\n",
    "        if enc_mode=='train':\n",
    "            cat_df_ohe=enc.fit_transform(cat_df)\n",
    "        elif enc_mode=='test': \n",
    "            cat_df_ohe=enc.transform(cat_df)\n",
    "        cat_df_ohe=pd.DataFrame(cat_df_ohe, columns=enc.get_feature_names_out())\n",
    "        cat_df_ohe[group_col]=df[group_col]\n",
    "        df_agg=cat_df_ohe.groupby(group_col).agg(['sum', 'mean'])\n",
    "\n",
    "        new_cols=[]\n",
    "        for l1 in df_agg.columns.levels[0]:\n",
    "            for l2 in ['count', 'count_norm']: # more suitable aliases for sum and mean\n",
    "                new_cols.append(f'{df_name}_{l1}_{l2}')\n",
    "        df_agg.columns=new_cols\n",
    "        if remove_dup:\n",
    "        # Remove duplicate columns by values\n",
    "            _, idx = np.unique(df_agg, axis = 1, return_index = True)\n",
    "            df_agg = df_agg.iloc[:, idx]\n",
    "        print(f'Dataset:{df_name}\\n\\tBefore: {cat_df.shape[1]} categorical cols\\n\\tAfter: {df_agg.shape[1]}')\n",
    "        return df_agg\n",
    "    else:\n",
    "        print('No categorical columns in dataframe')\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "def agg_combine(df, group_vars, df_names, enc, enc_mode='train', agg_level=1, remove_dup=True):\n",
    "    if agg_level==2:\n",
    "        df_cat_agg=categ_agg(df, group_vars[1], df_names[1], enc, enc_mode, remove_dup)\n",
    "        df_num_agg=numeric_agg(df, group_vars[1], df_names[1], remove_dup)\n",
    "        df_full_l2=df_cat_agg.merge(df_num_agg, on=group_vars[1], how='outer')\n",
    "        df_full_l2=df[group_vars].merge(df_full_l2, on=group_vars[1], how='right')\n",
    "        df_full=numeric_agg(df_full_l2, group_vars[0], df_names[0], remove_dup)\n",
    "        gc.enable()\n",
    "        del df_full_l2\n",
    "        gc.collect()\n",
    "    elif agg_level==1:\n",
    "        df_cat_agg=categ_agg(df, group_vars[0], df_names[0], enc, enc_mode, remove_dup)\n",
    "        df_num_agg=numeric_agg(df, group_vars[0], df_names[0], remove_dup)\n",
    "        df_full=df_cat_agg.merge(df_num_agg, on=group_vars[0], how='outer')\n",
    "    else:\n",
    "        return 'Select aggregation level 1 or 2'\n",
    "    gc.enable()\n",
    "    del df_cat_agg, df_num_agg\n",
    "    gc.collect()\n",
    "    return df_full\n",
    "\n",
    "\n",
    "def save_template(df, template_path):\n",
    "    temp=pd.DataFrame([{k:np.nan for k,v in zip(df.columns, range(len(df.columns)))}])\n",
    "    with open(template_path, 'wb') as f:\n",
    "        pickle.dump(temp, f)\n",
    "\n",
    "\n",
    "def application_data(path):\n",
    "    df=pd.read_csv(path)\n",
    "    print(f'Application data shape: {df.shape}')\n",
    "    df['DAYS_EMPLOYED']=df['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
    "    df=generate_domain_features(df)\n",
    "    return convert_types(df)\n",
    "\n",
    "\n",
    "\n",
    "def bureau_and_bb(bur_path, bb_path, enc_mode='test', bur_ohe=None, bb_ohe=None, remove_dup=True, sample=None, template_path=None):\n",
    "    bur, bb=pd.read_csv(bur_path), pd.read_csv(bb_path)\n",
    "    if sample is not None:\n",
    "        bur, bb=bur.astype(sample[0]), bb.astype(sample[1])\n",
    "    bur=convert_types(bur)\n",
    "    bb=convert_types(bb)\n",
    "    print(f'Bureau shape: {bur.shape}')\n",
    "    print(f'Bureau balance shape: {bb.shape}')\n",
    "    if not bb_ohe:\n",
    "        bb_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    bb_numeric_agg_df=numeric_agg(bb, 'SK_ID_BUREAU', 'bureau_balance', remove_dup=remove_dup)\n",
    "    bb_categ_agg_df=categ_agg(bb, 'SK_ID_BUREAU', 'bureau_balance', bb_ohe, enc_mode, remove_dup=remove_dup)\n",
    "    bb_full=bb_numeric_agg_df.merge(bb_categ_agg_df, on='SK_ID_BUREAU', how='outer')\n",
    "    bb_by_credit=bur[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bb_full, on='SK_ID_BUREAU', how='left') \n",
    "    bb=numeric_agg(bb_by_credit, 'SK_ID_CURR', 'loan', remove_dup=remove_dup)\n",
    "    if not bur_ohe:\n",
    "        bur_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    bur=agg_combine(bur, ['SK_ID_CURR'], ['bureau'], bur_ohe, enc_mode, remove_dup=remove_dup)\n",
    "    del bb_numeric_agg_df, bb_categ_agg_df, bb_full, bb_by_credit; gc.collect()\n",
    "    if template_path is not None:\n",
    "        save_template(bur, template_path[0])\n",
    "        save_template(bb, template_path[1])\n",
    "    return bur, bb, bur_ohe, bb_ohe\n",
    "\n",
    "\n",
    "\n",
    "def previous(prev_path, enc_mode='test', prev_ohe=None, remove_dup=True, sample=None, template_path=None):\n",
    "    prev=pd.read_csv(prev_path)\n",
    "    if sample is not None:\n",
    "        prev=prev.astype(sample)\n",
    "    prev=convert_types(prev)\n",
    "    print(f'Previous shape: {prev.shape}')\n",
    "    for c in ['DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION']:\n",
    "        prev[c]=prev[c].replace(365243, np.nan)\n",
    "    if not prev_ohe:\n",
    "        prev_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    prev=agg_combine(prev, ['SK_ID_CURR'], ['previous'], prev_ohe, enc_mode, remove_dup=remove_dup)\n",
    "    if template_path is not None:\n",
    "        save_template(prev, template_path)\n",
    "    return prev, prev_ohe\n",
    "\n",
    "\n",
    "\n",
    "def pos_cash(cash_path, enc_mode='test', cash_ohe=None, remove_dup=True, sample=None, template_path=None):\n",
    "    cash=pd.read_csv(cash_path)\n",
    "    if sample is not None:\n",
    "        cash=cash.astype(sample)\n",
    "    cash=convert_types(pd.read_csv(cash_path))\n",
    "    print(f'Cash shape: {cash.shape}')\n",
    "    if not cash_ohe:\n",
    "        cash_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    cash=agg_combine(cash, ['SK_ID_CURR', 'SK_ID_PREV'], ['loan', 'cash'], cash_ohe, enc_mode, agg_level=2, remove_dup=remove_dup)\n",
    "    if template_path is not None:\n",
    "        save_template(cash, template_path)\n",
    "    return cash, cash_ohe\n",
    "\n",
    "\n",
    "\n",
    "def installments(inst_path, remove_dup=True, sample=None, template_path=None):\n",
    "    inst=pd.read_csv(inst_path)\n",
    "    if sample is not None:\n",
    "        inst=inst.astype(sample)\n",
    "    inst=convert_types(inst)\n",
    "    print(f'Installments shape: {inst.shape}')\n",
    "    inst_agg_by_prev=numeric_agg(inst, 'SK_ID_PREV', 'inst', remove_dup=remove_dup)\n",
    "    inst_agg_by_prev=inst[['SK_ID_PREV', 'SK_ID_CURR']].merge(inst_agg_by_prev, on='SK_ID_PREV', how='right')\n",
    "    inst=numeric_agg(inst_agg_by_prev, 'SK_ID_CURR', 'loan', remove_dup=remove_dup)\n",
    "    if template_path is not None:\n",
    "        save_template(inst, template_path)\n",
    "    del inst_agg_by_prev; gc.collect()\n",
    "    return inst\n",
    "\n",
    "\n",
    "\n",
    "def card_balance(card_path, enc_mode='test', card_ohe=None, remove_dup=True, sample=None, template_path=None):\n",
    "    card_balance=pd.read_csv(card_path)\n",
    "    if sample is not None:\n",
    "        card_balance=card_balance.astype(sample)\n",
    "    card_balance=convert_types(card_balance)\n",
    "    print(f'Card Balance shape: {card_balance.shape}')\n",
    "    if not card_ohe:\n",
    "        card_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    card_balance=agg_combine(card_balance, ['SK_ID_CURR', 'SK_ID_PREV'], \n",
    "                            ['loan', 'card'], card_ohe, enc_mode, agg_level=2,\n",
    "                            remove_dup=remove_dup)\n",
    "    if template_path is not None:\n",
    "        save_template(card_balance, template_path)\n",
    "    return card_balance, card_ohe\n",
    "\n",
    "\n",
    "\n",
    "def full_df(path_dict, mode='train'):\n",
    "    if mode=='train':\n",
    "        ohe_dict={}\n",
    "        app=application_data(path_dict['application_train'])\n",
    "        bur, bb, bur_ohe, bb_ohe=bureau_and_bb(path_dict['bur'], path_dict['bb'], \n",
    "                                                template_path=[path_dict['bur_temp'], path_dict['bb_temp']])\n",
    "        app=app.merge(bur, on='SK_ID_CURR', how='left')\n",
    "        ohe_dict['Bureau_OHE']=bur_ohe\n",
    "        ohe_dict['BB_OHE']= bb_ohe\n",
    "        del bur, bur_ohe; gc.collect()\n",
    "        app=app.merge(bb, on='SK_ID_CURR', how='left')\n",
    "        del bb, bb_ohe; gc.collect()\n",
    "        \n",
    "        prev, prev_ohe=previous(path_dict['previous'], template_path=path_dict['prev_temp'])\n",
    "        app=app.merge(prev, on='SK_ID_CURR', how='left')\n",
    "        ohe_dict['Prev_OHE']= prev_ohe\n",
    "        del prev, prev_ohe; gc.collect()\n",
    "        \n",
    "        cash, cash_ohe=pos_cash(path_dict['cash'], template_path=path_dict['cash_temp'])\n",
    "        app=app.merge(cash, on='SK_ID_CURR', how='left')\n",
    "        ohe_dict['Cash_OHE']= cash_ohe\n",
    "        del cash, cash_ohe; gc.collect()\n",
    "        \n",
    "        inst=installments(path_dict['installments'], template_path=path_dict['inst_temp'])\n",
    "        app=app.merge(inst, on='SK_ID_CURR', how='left')\n",
    "        del inst; gc.collect()\n",
    "        \n",
    "        card_b, card_b_ohe=card_balance(path_dict['card_balance'], template_path=path_dict['card_temp'])\n",
    "        app=app.merge(card_b, on='SK_ID_CURR', how='left')\n",
    "        ohe_dict['Card_OHE']=card_b_ohe\n",
    "        del card_b, card_b_ohe; gc.collect()\n",
    "       \n",
    "        with open(path_dict['ohe_dict'], 'wb') as f:\n",
    "            pickle.dump(ohe_dict, f)\n",
    "        \n",
    "    elif mode=='test':\n",
    "        with open(path_dict['ohe_dict'], 'rb') as f:\n",
    "            ohe_dict=pickle.load(f)\n",
    "        app=application_data(path_dict['application_test'])\n",
    "        bur, bb, _, _=bureau_and_bb(path_dict['bur'], \n",
    "                                    path_dict['bb'], \n",
    "                                    bur_ohe=ohe_dict['Bureau_OHE'], \n",
    "                                    bb_ohe=ohe_dict['BB_OHE'])\n",
    "        app=app.merge(bur, on='SK_ID_CURR', how='left')\n",
    "        del bur; gc.collect()\n",
    "        app=app.merge(bb, on='SK_ID_CURR', how='left')\n",
    "        del bb; gc.collect()\n",
    "        \n",
    "        prev, _=previous(path_dict['previous'], prev_ohe=ohe_dict['Prev_OHE'])\n",
    "        app=app.merge(prev, on='SK_ID_CURR', how='left')\n",
    "        del prev; gc.collect()\n",
    "        \n",
    "        cash, _=pos_cash(path_dict['cash'], cash_ohe=ohe_dict['Cash_OHE'])\n",
    "        app=app.merge(cash, on='SK_ID_CURR', how='left')\n",
    "        del cash; gc.collect()\n",
    "        \n",
    "        inst=installments(path_dict['installments'])\n",
    "        app=app.merge(inst, on='SK_ID_CURR', how='left')\n",
    "        del inst; gc.collect()\n",
    "        \n",
    "        card_b, _=card_balance(path_dict['card_balance'], card_ohe=ohe_dict['Card_OHE'])\n",
    "        app=app.merge(card_b, on='SK_ID_CURR', how='left')\n",
    "        del card_b; gc.collect()\n",
    "    else:\n",
    "        raise Exception(\"Specify mode (train of test) and OHE dict bool\")\n",
    "\n",
    "    app=app.rename(columns=lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    app=convert_types(app)\n",
    "\n",
    "    return app\n",
    "\n",
    "\n",
    "\n",
    "def correlation_filter(df, thresh, corr_path, mode='train'):\n",
    "    if mode=='train':\n",
    "        corr_mat=df.drop(['SK_ID_CURR', 'TARGET'], axis=1).corr().abs()\n",
    "        upper=corr_mat.where(np.triu(np.ones(corr_mat.shape), k=1).astype(np.bool))\n",
    "        upper.to_csv(corr_path, index=False)\n",
    "    elif mode=='test':\n",
    "        upper=pd.read_csv(corr_path)\n",
    "    to_drop=[column for column in upper.columns if any(upper[column]>thresh)]\n",
    "    print(f'Correlation: {len(to_drop)} will be removed')\n",
    "    return df.drop(to_drop, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def missing_filter(df, thresh, col_path, mode='train'):\n",
    "    if mode=='train':\n",
    "        mt=miss_table(df)\n",
    "        to_drop=mt.loc[mt['Percent']>thresh].index\n",
    "        with open(col_path, 'wb') as f:\n",
    "            pickle.dump(to_drop, f)\n",
    "    elif mode=='test':\n",
    "        with open(col_path, 'rb') as f:\n",
    "            to_drop=pickle.load(f)\n",
    "    else:\n",
    "        raise Exception(\"Specify mode (train of test) and path\")\n",
    "    print(f'{len(to_drop)} features with {thresh}% of NaNs will be removed')\n",
    "    return df.drop(to_drop, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def zero_var_filter(df, col_path, mode='train'):\n",
    "    if mode=='train':\n",
    "        scaler=MinMaxScaler()\n",
    "        numeric_data=df.drop('SK_ID_CURR', axis=1).select_dtypes('number').reset_index(drop=True)\n",
    "        numeric_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        for i in numeric_data.columns:\n",
    "            numeric_data[i].fillna(value=numeric_data[i].mean(), inplace=True) #replace NaN with mean of dimension\n",
    "            numeric_data[i]=scaler.fit_transform(numeric_data[i].values.reshape(-1,1)) \n",
    "        vars_df=numeric_data.var()\n",
    "        to_drop=vars_df[vars_df==0].index\n",
    "        with open(col_path, 'wb') as f:\n",
    "            pickle.dump(to_drop, f)\n",
    "    \n",
    "    elif mode=='test':\n",
    "        with open(col_path, 'rb') as f:\n",
    "            to_drop=pickle.load(f)\n",
    "    else:\n",
    "        raise Exception(\"Specify mode (train of test) and path\")\n",
    "    print(f'{len(to_drop)} features with zero variance will be removed')\n",
    "    return df.drop(to_drop, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def feat_imp_cv(df, feat_imp_path, k=5, params=None, mode='train'):\n",
    "    if mode=='train':\n",
    "        data=df.copy()\n",
    "        for c in data:\n",
    "            if (data[c].dtype=='object') and (data[c].nunique()<data.shape[0]):\n",
    "                data[c]=data[c].astype('category')\n",
    "        X, y=data.drop(['SK_ID_CURR', 'TARGET'], axis=1), data['TARGET']\n",
    "        feat_importances_gain, feat_importances_split=[], []\n",
    "        cols=list(X.columns)\n",
    "        cat_feats=list(X.select_dtypes(['category']).columns)\n",
    "        kfold=StratifiedKFold(k)\n",
    "        for f, (tr, te) in enumerate(kfold.split(X, y=y)):\n",
    "            X_train, y_train=X.iloc[tr, :], y.iloc[tr]\n",
    "            X_test, y_test=X.iloc[te, :], y.iloc[te]\n",
    "            weight=np.count_nonzero(y_train==0)/np.count_nonzero(y_train==1)\n",
    "            params['scale_pos_weight']=weight\n",
    "            dtrain=lgb.Dataset(X_train, label=y_train, params={'verbose': -1})\n",
    "            dval=lgb.Dataset(X_test, label=y_test, params={'verbose': -1})\n",
    "            model=lgb.train(\n",
    "                            params=params,\n",
    "                            train_set=dtrain,\n",
    "                            valid_sets=[dtrain, dval],\n",
    "                            valid_names=['train', 'test'],\n",
    "                            categorical_feature=cat_feats,\n",
    "                            callbacks=[lgb.early_stopping(100, verbose=-1)],\n",
    "                            verbose_eval=False\n",
    "                            )\n",
    "            feat_importances_gain.append(model.feature_importance(importance_type='gain'))\n",
    "            feat_importances_split.append(model.feature_importance(importance_type='split'))\n",
    "        \n",
    "        gc.enable(); del data, X, y; gc.collect()\n",
    "        feat_importances_gain=np.array(feat_importances_gain).mean(axis=0)\n",
    "        feat_importances_split=np.array(feat_importances_split).mean(axis=0)\n",
    "        feat_importances_df=pd.DataFrame({'feature': cols,\n",
    "                                        'importance (gain)': feat_importances_gain,\n",
    "                                        'importance (split)': feat_importances_split,})\n",
    "        with open(feat_imp_path, 'wb') as f:\n",
    "            pickle.dump(feat_importances_df, f)\n",
    "    \n",
    "    elif mode=='test':\n",
    "        with open(feat_imp_path, 'rb') as f:\n",
    "            feat_importances_df=pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"Specify mode (train of test) and path\")\n",
    "    return feat_importances_df\n",
    "\n",
    "\n",
    "\n",
    "def drop_zero_imp(df, feat_imp_path, k=5, params=None, mode='train', drop_by='importance (gain)'):\n",
    "    feature_imp_df=feat_imp_cv(df, feat_imp_path, k, params, mode=mode)\n",
    "    to_drop=feature_imp_df[feature_imp_df[drop_by]==0]['feature'].values\n",
    "    print(f'Num of features with zero importance: {len(to_drop)}')\n",
    "    return df.drop(to_drop, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def save_data(df, df_path, dtypes_path=None):\n",
    "    df.to_csv(df_path, index=False)\n",
    "    if dtypes_path:\n",
    "        with open(dtypes_path, 'wb') as f:\n",
    "            pickle.dump(df.dtypes, f)\n",
    "    print(f'Size: {round(sys.getsizeof(df) / 1e9, 2)}gb\\nShape: {df.shape}\\nSaved to: {df_path}')\n",
    "\n",
    "\n",
    "\n",
    "def save_dtypes(df_names, path_dict, dtypes_dict):\n",
    "    for i in df_names:\n",
    "        df=pd.read_csv(path_dict[i])\n",
    "        with open(dtypes_dict[i], 'wb') as f:\n",
    "            pickle.dump(df.dtypes, f)\n",
    "    gc.enable(); del df; gc.collect()\n",
    "    return 'Dtypes Saved'\n",
    "\n",
    "\n",
    "\n",
    "def train_model(df, params, model_path, col_tran_path):\n",
    "    X, y=df.drop(['SK_ID_CURR', 'TARGET'], axis=1), df['TARGET']\n",
    "    cat_cols, num_cols=X.select_dtypes(include=['object']).columns, X.select_dtypes('number').columns\n",
    "    ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "    col_tran=ColumnTransformer([\n",
    "        ('cat', ohe, cat_cols),\n",
    "        ('num', 'passthrough', num_cols)\n",
    "    ])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=52, stratify=y)\n",
    "    X_train=col_tran.fit_transform(X_train)\n",
    "    X_val=col_tran.transform(X_val)\n",
    "    with open(col_tran_path, 'wb') as f:\n",
    "            pickle.dump(col_tran, f)\n",
    "    dtrain=lgb.Dataset(X_train, label=y_train, params={'verbose': -1})\n",
    "    dval=lgb.Dataset(X_val, label=y_val, params={'verbose': -1})\n",
    "    weight=np.count_nonzero(y==0)/np.count_nonzero(y==1)\n",
    "    params['scale_pos_weight']=weight\n",
    "    model=lgb.train(\n",
    "                    params=params,\n",
    "                    train_set=dtrain,\n",
    "                    valid_sets=[dtrain, dval],\n",
    "                    valid_names=['train', 'test'],\n",
    "                    callbacks=[lgb.early_stopping(100, verbose=-1)],\n",
    "                    verbose_eval=False\n",
    "                    )\n",
    "    model.save_model(model_path)\n",
    "    \n",
    "    return f'Model is saved in {model_path}'\n",
    "\n",
    "\n",
    "\n",
    "def make_prediction(X, model_path, col_tran_path, save_path=None):\n",
    "    with open(col_tran_path, 'rb') as f:\n",
    "        col_tran=pickle.load(f)\n",
    "    model=lgb.Booster(model_file=model_path)\n",
    "    pred=model.predict(col_tran.transform(X.drop('SK_ID_CURR', axis=1)))\n",
    "    if save_path:\n",
    "        submit=X[['SK_ID_CURR']]\n",
    "        submit.loc[:, 'TARGET']=pred\n",
    "        submit.to_csv(save_path, index=False)\n",
    "        return submit\n",
    "    return pred\n",
    "\n",
    "\n",
    "\n",
    "def timer():\n",
    "    print(f'Current time: {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "\n",
    "\n",
    "def read_sample(data, nrows=None):\n",
    "    s=data.to_json(orient='records')\n",
    "    s=json.loads(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def predict_api(data, dtypes_path, model_path, ohe_path):\n",
    "    with open(ohe_path, 'rb') as f:\n",
    "        ohe=pickle.load(f)\n",
    "    model=lgb.Booster(model_file=model_path)\n",
    "    df=pd.DataFrame([data])\n",
    "    pred=model.predict(ohe.transform(df.drop('SK_ID_CURR', axis=1)))\n",
    "    return pred[0]\n",
    "\n",
    "\n",
    "\n",
    "def create_sample(client_id, path_dict, save_dict):\n",
    "    app=pd.read_csv(path_dict['application_test'])\n",
    "    client_app=app.loc[app['SK_ID_CURR']==client_id, :].astype(app.dtypes)\n",
    "    client_app.to_csv(save_dict['application_test'], index=False)\n",
    "    print(f'app: {client_app.shape}')\n",
    "    gc.enable(); del app, client_app; gc.collect()\n",
    "\n",
    "    bur=pd.read_csv(path_dict['bur'])\n",
    "    client_bur=bur.loc[bur['SK_ID_CURR']==client_id, :].astype(bur.dtypes)\n",
    "    client_bur.to_csv(save_dict['bur'], index=False)\n",
    "    print(f'bur: {client_bur.shape}')\n",
    "    del bur; gc.collect()\n",
    "\n",
    "    bb=pd.read_csv(path_dict['bb'])\n",
    "    client_bb=bb.loc[bb['SK_ID_BUREAU'].isin(client_bur['SK_ID_BUREAU']), :].astype(bb.dtypes)\n",
    "    client_bb.to_csv(save_dict['bb'], index=False)\n",
    "    print(f'bb: {client_bb.shape}')\n",
    "    del bb, client_bb, client_bur; gc.collect()\n",
    "\n",
    "    prev=pd.read_csv(path_dict['previous'])\n",
    "    client_prev=prev.loc[prev['SK_ID_CURR']==client_id, :].astype(prev.dtypes)\n",
    "    client_prev.to_csv(save_dict['previous'], index=False)\n",
    "    print(f'prev: {client_prev.shape}')\n",
    "    del prev, client_prev; gc.collect()\n",
    "    \n",
    "    inst=pd.read_csv(config.PATH_DICT['installments'])\n",
    "    client_inst=inst.loc[inst['SK_ID_CURR']==client_id, :].astype(inst.dtypes)\n",
    "    client_inst.to_csv(save_dict['installments'], index=False)\n",
    "    print(f'inst: {client_inst.shape}')\n",
    "    del inst, client_inst; gc.collect()\n",
    "\n",
    "    cash=pd.read_csv(config.PATH_DICT['cash'])\n",
    "    client_cash=cash.loc[cash['SK_ID_CURR']==client_id, :].astype(cash.dtypes)\n",
    "    client_cash.to_csv(save_dict['cash'], index=False)\n",
    "    print(f'cash: {client_cash.shape}')\n",
    "    del cash, client_cash; gc.collect()\n",
    "\n",
    "    card=pd.read_csv(config.PATH_DICT['card_balance'])\n",
    "    client_card=card.loc[card['SK_ID_CURR']==client_id, :].astype(card.dtypes)\n",
    "    client_card.to_csv(save_dict['card_balance'], index=False)\n",
    "    print(f'card: {client_card.shape}')\n",
    "    del card, client_card; gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "def full_sample(client_id, samples_dict, path_dict, sample_dtypes_path):\n",
    "    with open(path_dict['ohe_dict'], 'rb') as f:\n",
    "        ohe_dict=pickle.load(f)\n",
    "        \n",
    "    with open(sample_dtypes_path, 'rb') as f:\n",
    "        orig_dtypes_dict=pickle.load(f)\n",
    "        \n",
    "    app=application_data(samples_dict['application_test']).astype(orig_dtypes_dict['application_train'].drop('TARGET'))\n",
    "    try:\n",
    "        bur, bb, _, _=bureau_and_bb(samples_dict['bur'], \n",
    "                                    samples_dict['bb'], \n",
    "                                    bur_ohe=ohe_dict['Bureau_OHE'], \n",
    "                                    bb_ohe=ohe_dict['BB_OHE'],\n",
    "                                    remove_dup=False, sample=[orig_dtypes_dict['bur'], orig_dtypes_dict['bb']])\n",
    "        bur.rename(columns=lambda s: s.replace(\" \", \"\"), inplace=True)\n",
    "        bb.rename(columns=lambda s: s.replace(\" \", \"\"), inplace=True)\n",
    "        app=app.merge(bur, on='SK_ID_CURR', how='left')\n",
    "        del bur; gc.collect()\n",
    "        app=app.merge(bb, on='SK_ID_CURR', how='left')\n",
    "        del bb; gc.collect()\n",
    "    except ValueError as e:\n",
    "        print(f'{e}')\n",
    "       \n",
    "    \n",
    "    try:\n",
    "        prev, _=previous(samples_dict['previous'], prev_ohe=ohe_dict['Prev_OHE'], remove_dup=False, sample=orig_dtypes_dict['previous'])\n",
    "    except ValueError as e:\n",
    "        with open(path_dict['prev_temp'], 'rb') as f:\n",
    "            prev=pickle.load(f)\n",
    "        prev['SK_ID_CURR']=client_id\n",
    "    for i in [' ', '-', ':', ')', '(', '+', '/', ',']:\n",
    "        prev.rename(columns=lambda s: s.replace(i, \"\"), inplace=True)\n",
    "    app=app.merge(prev, on='SK_ID_CURR', how='left')\n",
    "    del prev; gc.collect()\n",
    "      \n",
    "    \n",
    "    try:\n",
    "        cash, _=pos_cash(samples_dict['cash'], cash_ohe=ohe_dict['Cash_OHE'], remove_dup=False, sample=orig_dtypes_dict['cash'])\n",
    "    except ValueError as e:\n",
    "        with open(path_dict['cash_temp'], 'rb') as f:\n",
    "            cash=pickle.load(f)\n",
    "        cash['SK_ID_CURR']=client_id\n",
    "    cash.rename(columns=lambda s: s.replace(\" \", \"\"), inplace=True)\n",
    "    app=app.merge(cash, on='SK_ID_CURR', how='left')\n",
    "    del cash; gc.collect()\n",
    "      \n",
    "    \n",
    "    try:\n",
    "        inst=installments(samples_dict['installments'], remove_dup=False, sample=orig_dtypes_dict['installments'])\n",
    "    except ValueError as e:\n",
    "        with open(path_dict['inst_temp'], 'rb') as f:\n",
    "            inst=pickle.load(f)\n",
    "        inst['SK_ID_CURR']=client_id\n",
    "    inst.rename(columns=lambda s: s.replace(\" \", \"\"), inplace=True)\n",
    "    app=app.merge(inst, on='SK_ID_CURR', how='left')\n",
    "    del inst; gc.collect()\n",
    "    \n",
    "    \n",
    "    try:    \n",
    "        card_b, _=card_balance(samples_dict['card_balance'], card_ohe=ohe_dict['Card_OHE'], \n",
    "                              remove_dup=False, sample=orig_dtypes_dict['card_balance'])\n",
    "    except ValueError as e:\n",
    "        with open(path_dict['card_temp'], 'rb') as f:\n",
    "            card_b=pickle.load(f)\n",
    "        card_b['SK_ID_CURR']=client_id\n",
    "        \n",
    "    card_b.rename(columns=lambda s: s.replace(\" \", \"\"), inplace=True)\n",
    "    app=app.merge(card_b, on='SK_ID_CURR', how='left')\n",
    "    del card_b; gc.collect()\n",
    "    \n",
    "    with open(path_dict['dtypes'], 'rb') as f:\n",
    "        final_dtypes=pickle.load(f)\n",
    "        \n",
    "    return app[final_dtypes.drop('TARGET').index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52535db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import config\n",
    "import time\n",
    "\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "def full_train():\n",
    "    path_dict=config.PATH_DICT\n",
    "    params=config.PARAMS\n",
    "    print('TRAIN PIPELINE')\n",
    "    timer()\n",
    "    print('\\n*** 0. Saving Original Dtypes ***')\n",
    "    save_dtypes(list(config.DTYPES_DICT.keys()), config.PATH_DICT, config.DTYPES_DICT)\n",
    "    timer()\n",
    "    print('\\n*** 1. Feature Generation ***')\n",
    "    train=full_df(path_dict, \n",
    "                mode='train')\n",
    "    timer()\n",
    "    print('\\n*** 2. Correlation ***')\n",
    "    train=correlation_filter(train, \n",
    "                            thresh=0.9, \n",
    "                            corr_path=path_dict['corr_matrix'], \n",
    "                            mode='test')\n",
    "    timer()\n",
    "    print('\\n*** 3. Missing Variables ***')\n",
    "    train=missing_filter(train, \n",
    "                        thresh=80, \n",
    "                        col_path=path_dict['missing_columns_drop'], \n",
    "                        mode='train')\n",
    "    timer()\n",
    "    print('\\n*** 4. Zero Variance ***')\n",
    "    train=zero_var_filter(train, \n",
    "                        col_path=path_dict['zero_variance_drop'], \n",
    "                        mode='train')\n",
    "    timer()\n",
    "    print('\\n*** 5. Zero Importance ***')\n",
    "    train=drop_zero_imp(train, \n",
    "                        feat_imp_path=path_dict['zero_imp_drop'], \n",
    "                        k=5, \n",
    "                        params=params, \n",
    "                        mode='train', \n",
    "                        drop_by='importance (gain)')\n",
    "    timer()\n",
    "    save_data(train, path_dict['train_ready_file'], path_dict['dtypes'])\n",
    "    print('\\n*** 6. Model Training ***')\n",
    "    train_model(train, \n",
    "                params=params, \n",
    "                model_path=path_dict['model_file'],\n",
    "                col_tran_path=path_dict['lgb_ohe'])\n",
    "    \n",
    "    print(f'\\n*** DONE : {(time.time()-start_time)/60:.3f} Minutes ***')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    full_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d730ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import config\n",
    "import time\n",
    "\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "\n",
    "def full_test():\n",
    "    path_dict=config.PATH_DICT\n",
    "    print('TEST PIPELINE')\n",
    "    timer()\n",
    "    print('\\n*** 1. Feature Generation ***')\n",
    "    test=full_df(path_dict, \n",
    "                mode='test')\n",
    "    timer()\n",
    "    print('\\n*** 2. Correlation ***')\n",
    "    test=correlation_filter(test, \n",
    "                            thresh=0.9, \n",
    "                            corr_path=path_dict['corr_matrix'], \n",
    "                            mode='test')\n",
    "    timer()\n",
    "    print('\\n*** 3. Missing Variables ***')\n",
    "    test=missing_filter(test, \n",
    "                        thresh=80, \n",
    "                        col_path=path_dict['missing_columns_drop'], \n",
    "                        mode='test')\n",
    "    timer()\n",
    "    print('\\n*** 4. Zero 3 ***')\n",
    "    test=zero_var_filter(test, \n",
    "                        col_path=path_dict['zero_variance_drop'], \n",
    "                        mode='test')\n",
    "    timer()\n",
    "    print('\\n*** 5. Zero Importance ***')\n",
    "    test=drop_zero_imp(test, \n",
    "                        feat_imp_path=path_dict['zero_imp_drop'], \n",
    "                        k=5, \n",
    "                        params=None, \n",
    "                        mode='test', \n",
    "                        drop_by='importance (gain)')\n",
    "    timer()\n",
    "    save_data(test, path_dict['test_ready_file'])\n",
    "    print('\\n*** 6. Making Prediction ***')\n",
    "    make_prediction(test, \n",
    "                    model_path=path_dict['model_file'],\n",
    "                    col_tran_path=path_dict['lgb_ohe'],\n",
    "                    save_path=path_dict['submit'])\n",
    "        \n",
    "    print(f'\\n*** DONE : {(time.time()-start_time)/60:.2f} Minutes ***')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    full_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db383f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DICT={\n",
    "    'application_train': '../data/application_train.csv',\n",
    "    'application_test': '../data/application_test.csv',\n",
    "    'bur': '../data/bureau.csv',\n",
    "    'bb': '../data/bureau_balance.csv',\n",
    "    'previous': '../data/previous_application.csv',\n",
    "    'cash': '../data/POS_CASH_balance.csv',\n",
    "    'installments': '../data/installments_payments.csv',\n",
    "    'card_balance': '../data/credit_card_balance.csv',\n",
    "    'ohe_dict': '../models/pipeline/ohe_dict.pkl',\n",
    "    'corr_matrix': '../data/corr_matrix.csv',\n",
    "    'missing_columns_drop': '../models/pipeline/missing_columns_drop.pkl',\n",
    "    'zero_variance_drop': '../models/pipeline/zero_var_columns_drop.pkl',\n",
    "    'zero_imp_drop': '../models/pipeline/zero_imp.pkl',\n",
    "    'model_file': '../models/model.txt',\n",
    "    'lgb_ohe': '../models/pipeline/lgb_ohe.pkl',\n",
    "    'train_ready_file': '../data/train_ready.csv',\n",
    "    'test_ready_file': '../data/test_ready.csv',\n",
    "    'dtypes': '../models/pipeline/dtypes_final.pkl',\n",
    "    'submit': '../data/submit.csv',\n",
    "    'bur_temp': '../data/templates/bur_temp.pkl',\n",
    "    'bb_temp': '../data/templates/bb_temp.pkl',\n",
    "    'prev_temp': '../data/templates/prev_temp.pkl',\n",
    "    'cash_temp': '../data/templates/cash_temp.pkl',\n",
    "    'inst_temp': '../data/templates/inst_temp.pkl',\n",
    "    'card_temp': '../data/templates/card_temp.pkl',\n",
    "    }\n",
    "\n",
    "SAMPLES_DICT={\n",
    "    'application_test': '../data/samples/app_sample.csv',\n",
    "    'bur': '../data/samples/bureau_sample.csv',\n",
    "    'bb': '../data/samples/bb_sample.csv',\n",
    "    'previous': '../data/samples/previous_sample.csv',\n",
    "    'cash': '../data/samples/cash_sample.csv',\n",
    "    'installments': '../data/samples/inst_sample.csv',\n",
    "    'card_balance': '../data/samples/card_sample.csv',\n",
    "    'ohe_dict': '../models/pipeline/ohe_dict.pkl',\n",
    "}\n",
    "\n",
    "\n",
    "DTYPES_DICT={\n",
    "    'application_train': '../models/pipeline/dtypes/app_dtypes.pkl',\n",
    "    'bur': '../models/pipeline/dtypes/bureau_dtypes.pkl',\n",
    "    'bb': '../models/pipeline/dtypes/bb_dtypes.pkl',\n",
    "    'previous': '../models/pipeline/dtypes/previous_dtypes.pkl',\n",
    "    'cash': '../models/pipeline/dtypes/cash_dtypes.pkl',\n",
    "    'installments': '../models/pipeline/dtypes/inst_dtypes.pkl',\n",
    "    'card_balance': '../models/pipeline/dtypes/card_dtypes.pkl',\n",
    "}\n",
    "\n",
    "PARAMS={\n",
    "    'num_boost_round': 10000,\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.05,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 5,\n",
    "    'verbose': -1\n",
    "    }\n",
    "\n",
    "CLIENT_ID=100001\n",
    "\n",
    "CLIENTS_FILE='../data/clients/'\n",
    "\n",
    "CLIENT_FILENAMES=['application.csv', \n",
    "                    'bureau.csv', \n",
    "                    'bureau_balance.csv', \n",
    "                    'previous_app.csv', \n",
    "                    'cash.csv', \n",
    "                    'installments.csv', \n",
    "                    'card.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1464159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../src')\n",
    "from fastapi import FastAPI, Request\n",
    "import uvicorn\n",
    "import config\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "\n",
    "path_dict=config.PATH_DICT\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {'Description': 'API for calculating probability of default'}\n",
    "\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def post_prediction(data: Request):\n",
    "    cient_data=await data.json()\n",
    "    prediction=predict_api(cient_data[0],\n",
    "                            path_dict['dtypes'],\n",
    "                            path_dict['model_file'],\n",
    "                            path_dict['lgb_ohe'])\n",
    "    \n",
    "    return {\n",
    "        \"prediction\" : prediction\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    uvicorn.run('backend:app', host='0.0.0.0', port=8000, reload=True)\n",
    "\n",
    "# python -m uvicorn main:app --reload  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2396ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import sys\n",
    "sys.path.insert(1, '../src')\n",
    "import config\n",
    "import requests\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "st.title('Credit Risk Modelling')\n",
    "st.write('''Predicting probability of default of an applicant using various data sources. \n",
    "            You must upload 7 files with exact names as specified in description below.''')\n",
    "with st.expander(\"Data sources description.\"):\n",
    "        st.write('''\n",
    "        1. application: the main data with information about each loan application.\n",
    "        \n",
    "        2. bureau: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits.\n",
    "        \n",
    "        3. bureau_balance: monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length.\n",
    "        \n",
    "        4. previous_app: previous applications for loans of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.\n",
    "        \n",
    "        5. cash: monthly data about previous point of sale or cash loans clients have had. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n",
    "        \n",
    "        6. card: monthly data about previous credit cards clients have had. Each row is one month of a credit card balance, and a single credit card can have many rows.\n",
    "        \n",
    "        7. installments: payment history for previous loans. There is one row for every made payment and one row for every missed payment.\n",
    "        ''')\n",
    "\n",
    "test_data=st.file_uploader(\"All Data\", type='csv', accept_multiple_files =True)\n",
    "\n",
    "\n",
    "cutoff=st.slider('Choose your cutoff value (minimum probability that would be considered a \"default\", i.e., class-1)', 0., 1., 0.5)\n",
    "submit=st.button('Submit')\n",
    "if submit:\n",
    "    test_dict={i.name: i for i in test_data}\n",
    "    with st.spinner('Predicting...'):\n",
    "        if len(test_data)!=7:\n",
    "            st.error(\"Please provide all required information!\")\n",
    "        else:\n",
    "            app_df=pd.read_csv(test_dict['application.csv'])\n",
    "            del test_dict['application.csv']\n",
    "            client_id=app_df['SK_ID_CURR'].values[0]\n",
    "            client_dir=config.CLIENTS_FILE+str(client_id)\n",
    "            if not os.path.exists(client_dir):\n",
    "                os.makedirs(client_dir)\n",
    "            app_df.to_csv(client_dir+str('/application.csv'), index=False)\n",
    "            \n",
    "            file_names=config.CLIENT_FILENAMES\n",
    "\n",
    "            for fname, d in test_dict.items():\n",
    "                print(fname, d)\n",
    "                df=pd.read_csv(d)\n",
    "                df.to_csv(os.path.join(client_dir, fname), index=False)\n",
    "\n",
    "            client_data_dict=dict(zip(config.SAMPLES_DICT.keys(), [f'../data/clients/{client_id}/'+dir_ for dir_ in config.CLIENT_FILENAMES]))\n",
    "            \n",
    "            data_df=full_sample(client_id, client_data_dict, config.PATH_DICT, '../models/pipeline/sample_dtypes_dict.pkl')\n",
    "\n",
    "            data=read_sample(data_df)\n",
    "            response=requests.post('http://127.0.0.1:8000/predict', json=data)\n",
    "            pred=json.loads(response.text)['prediction']\n",
    "    st.write(f'Probability of Default is {pred:.3f}')\n",
    "    if pred>cutoff:\n",
    "        st.error(f'The applicant is likely to default!')\n",
    "    else:\n",
    "        st.success(f'The applicant is not likely to default:)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
