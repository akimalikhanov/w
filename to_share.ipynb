{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869b8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uTILS.PY\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import normaltest\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import lightgbm as lgb\n",
    "import sys\n",
    "import gc\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def return_size(df):\n",
    "    \"\"\"Return size of dataframe in gigabytes\"\"\"\n",
    "    return round(sys.getsizeof(df) / 1e9, 2)\n",
    "\n",
    "def convert_types(df, print_info = False):\n",
    "    \n",
    "    original_memory = df.memory_usage().sum()\n",
    "    \n",
    "    for c in df:\n",
    "        \n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "            \n",
    "        # elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "        #     df[c] = df[c].astype('category')\n",
    "        \n",
    "        elif list(df[c].unique()) == [1, 0]:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        \n",
    "        elif df[c].dtype == np.float64:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        elif df[c].dtype == np.int64:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    new_memory = df.memory_usage().sum()\n",
    "    \n",
    "    if print_info:\n",
    "        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n",
    "        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def miss_table(data):\n",
    "    miss_table=data.isna().sum().to_frame(name='Count')\n",
    "    miss_table['Percent']=miss_table['Count']/len(data)*100\n",
    "    miss_table['Dtype']=data.dtypes[miss_table.index]\n",
    "    miss_table['Count']=miss_table['Count'].replace({0: np.nan})\n",
    "    miss_table=miss_table.dropna()\n",
    "    print(f\"There are {len(miss_table)}/{data.shape[1]} columns with missing values\")\n",
    "    print('Distribution by dtypes:')\n",
    "    print(miss_table['Dtype'].value_counts())\n",
    "    return miss_table.sort_values(by='Count', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "def generate_domain_features(df):\n",
    "    # CREDIT related \n",
    "    bins=[18, 35, 40, 50, 60, 70, 120]\n",
    "    labels=['18-29', '30-39', '40-49', '50-59', '60-69', '70+']\n",
    "    df['NEW_AGE_GROUP']=pd.cut(df['DAYS_BIRTH']/-365, bins=bins, labels=labels, right=False)\n",
    "\n",
    "    cred_by_contract=df.groupby('NAME_CONTRACT_TYPE')['AMT_CREDIT'].mean() \n",
    "    cred_by_housing_type=df.groupby('NAME_HOUSING_TYPE')['AMT_CREDIT'].mean() \n",
    "    cred_by_org_type=df.groupby('ORGANIZATION_TYPE')['AMT_CREDIT'].mean() \n",
    "    cred_by_education_type=df.groupby('NAME_EDUCATION_TYPE')['AMT_CREDIT'].mean() \n",
    "    cred_by_gender=df.groupby('CODE_GENDER')['AMT_CREDIT'].mean() \n",
    "    cred_by_family_status=df.groupby('NAME_FAMILY_STATUS')['AMT_CREDIT'].mean()\n",
    "    cred_by_age_group=df.groupby('NEW_AGE_GROUP')['AMT_CREDIT'].mean()\n",
    "\n",
    "    df['NEW_AMT_CREDIT_TO_AMT_INCOME']=df['AMT_CREDIT']/df['AMT_INCOME_TOTAL'] \n",
    "    df['NEW_AMT_CREDIT_TO_AMT_ANNUITY']=df['AMT_CREDIT']/df['AMT_ANNUITY']\n",
    "    df['NEW_AMT_CREDIT_TO_AMT_GOODS_PRICE']=df['AMT_CREDIT']/df['AMT_GOODS_PRICE']\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_CREDIT_BY_CONTRACT_TYPE']=df['AMT_CREDIT']/(df['NAME_CONTRACT_TYPE'].map(cred_by_contract))\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_CREDIT_BY_HOUSING_TYPE']=df['AMT_CREDIT']/(df['NAME_HOUSING_TYPE'].map(cred_by_housing_type))\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_CREDIT_BY_ORGANIZATION_TYPE']=df['AMT_CREDIT']/(df['ORGANIZATION_TYPE'].map(cred_by_org_type))\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_CREDIT_BY_EDUCATION_TYPE']=df['AMT_CREDIT']/(df['NAME_EDUCATION_TYPE'].map(cred_by_education_type))\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_CREDIT_BY_GENDER']=df['AMT_CREDIT']/(df['CODE_GENDER'].map(cred_by_gender))\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_CREDIT_BY_FAMILY_STATUS']=df['AMT_CREDIT']/(df['NAME_FAMILY_STATUS'].map(cred_by_family_status))\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_INCOME_BY_AGE_GROUP']=df['AMT_CREDIT']/df['NEW_AGE_GROUP'].map(cred_by_age_group)\n",
    "\n",
    "\n",
    "    # INCOME related\n",
    "    inc_by_contract=df.groupby('NAME_CONTRACT_TYPE')['AMT_INCOME_TOTAL'].mean() \n",
    "    inc_by_housing_type=df.groupby('NAME_HOUSING_TYPE')['AMT_INCOME_TOTAL'].mean() \n",
    "    inc_by_org_type=df.groupby('ORGANIZATION_TYPE')['AMT_INCOME_TOTAL'].mean() \n",
    "    inc_by_education_type=df.groupby('NAME_EDUCATION_TYPE')['AMT_INCOME_TOTAL'].mean() \n",
    "    inc_by_gender=df.groupby('CODE_GENDER')['AMT_INCOME_TOTAL'].mean()\n",
    "    inc_by_family_status=df.groupby('NAME_FAMILY_STATUS')['AMT_INCOME_TOTAL'].mean()\n",
    "    inc_by_age_group=df.groupby('NEW_AGE_GROUP')['AMT_INCOME_TOTAL'].mean()\n",
    "\n",
    "    df['NEW_AMT_INCOME_BY_AGE_GROUP']=df['AMT_INCOME_TOTAL']/df['NEW_AGE_GROUP'].map(inc_by_age_group)\n",
    "    df['NEW_AMT_INCOME_BY_CNT_CHILD']=df['AMT_INCOME_TOTAL']/(1+df['CNT_CHILDREN'])\n",
    "    df['NEW_AMT_INCOME_BY_CNT_FAM_MEMBERS']=df['AMT_INCOME_TOTAL']/df['CNT_FAM_MEMBERS']\n",
    "    df['NEW_AMT_INCOME_BY_AGE']=df['AMT_INCOME_TOTAL']/(df['DAYS_BIRTH']/-365)\n",
    "    df['NEW_AMT_INCOME_TO_MEAN_AMT_CREDIT_BY_CONTRACT_TYPE']=df['AMT_INCOME_TOTAL']/(df['NAME_CONTRACT_TYPE'].map(inc_by_contract))\n",
    "    df['NEW_AMT_INCOME_TO_MEAN_AMT_CREDIT_BY_HOUSING_TYPE']=df['AMT_INCOME_TOTAL']/(df['NAME_HOUSING_TYPE'].map(inc_by_housing_type))\n",
    "    df['NEW_AMT_INCOME_TO_MEAN_AMT_CREDIT_BY_ORGANIZATION_TYPE']=df['AMT_INCOME_TOTAL']/(df['ORGANIZATION_TYPE'].map(inc_by_org_type))\n",
    "    df['NEW_AMT_INCOME_TO_MEAN_AMT_CREDIT_BY_EDUCATION_TYPE']=df['AMT_INCOME_TOTAL']/(df['NAME_EDUCATION_TYPE'].map(inc_by_education_type))\n",
    "    df['NEW_AMT_INCOME_TO_MEAN_AMT_CREDIT_BY_GENDER']=df['AMT_INCOME_TOTAL']/(df['CODE_GENDER'].map(inc_by_gender))\n",
    "    df['NEW_AMT_CREDIT_TO_MEAN_AMT_CREDIT_BY_FAMILY_STATUS']=df['AMT_CREDIT']/(df['NAME_FAMILY_STATUS'].map(inc_by_family_status))\n",
    "    df['NEW_AMT_INCOME_TO_MEAN_AMT_INCOME_BY_AGE_GROUP']=df['AMT_INCOME_TOTAL']/df['NEW_AGE_GROUP'].map(inc_by_age_group)\n",
    "\n",
    "\n",
    "    # FLAG related\n",
    "    # doc_flags--20 columns about documents\n",
    "    # contact_flags--6 flags about contact info of client (FLAG_MOBIL, FLAG_EMAIL, etc)\n",
    "    # address_flags--6 flags about address info of client (REG_REGION_NOT_LIVE_REGION, REG_REGION_NOT_WORK_REGION, etc)\n",
    "    doc_flags=[i for i in df.columns if 'FLAG_DOCUMENT' in i]\n",
    "    contact_flags=[i for i in df.columns if ('FLAG' in i) and (i not in doc_flags) and (i not in ('FLAG_OWN_CAR', 'FLAG_OWN_REALTY'))]\n",
    "    address_flags=[i for i in df.columns if 'NOT' in i]\n",
    "    flag_map={'Y':1, 'N':0}\n",
    "\n",
    "    df['NEW_DOC_FLAG_MEAN']=df[doc_flags].mean(axis=1)\n",
    "    df['NEW_DOC_FLAG_SUM']=df[doc_flags].sum(axis=1)\n",
    "    df['NEW_CONTACT_FLAG_MEAN']=df[contact_flags].mean(axis=1)\n",
    "    df['NEW_CONTACT_FLAG_SUM']=df[contact_flags].sum(axis=1)\n",
    "    df['NEW_ADDRESS_FLAG_MEAN']=df[address_flags].mean(axis=1)\n",
    "    df['NEW_ADDRESS_FLAG_SUM']=df[address_flags].sum(axis=1)\n",
    "    df['NEW_OWN_CAR_REALTY_COMBINATION']=0.75*df['FLAG_OWN_REALTY'].map(flag_map)+0.25*df['FLAG_OWN_CAR'].map(flag_map)\n",
    "\n",
    "\n",
    "    # AGE related\n",
    "    age_by_housing_type=df.groupby('NAME_HOUSING_TYPE')['DAYS_BIRTH'].mean()\n",
    "    age_by_own_realty=df.groupby('FLAG_OWN_REALTY')['DAYS_BIRTH'].mean()\n",
    "    age_by_own_car=df.groupby('FLAG_OWN_CAR')['DAYS_BIRTH'].mean()\n",
    "\n",
    "    df['NEW_AGE_TO_MEAN_AGE_BY_FLAG_OWN_REALTY']=df['DAYS_BIRTH']/(df['FLAG_OWN_REALTY'].map(age_by_own_realty))\n",
    "    df['NEW_AGE_TO_MEAN_AGE_BY_FLAG_OWN_CAR']=df['DAYS_BIRTH']/(df['FLAG_OWN_CAR'].map(age_by_own_car))\n",
    "    df['NEW_AGE_TO_MEAN_AGE_BY_HOUSING_TYPE']=df['DAYS_BIRTH']/(df['NAME_HOUSING_TYPE'].map(age_by_housing_type))\n",
    "    df[\"NEW_DAYS_EMPLOYED_TO_DAYS_BIRTH\"]=df['DAYS_EMPLOYED']/df['DAYS_BIRTH']\n",
    "    df[\"NEW_DAYS_REGISTRATION_TO_DAYS_BIRTH\"]=df['DAYS_REGISTRATION']/df['DAYS_BIRTH']\n",
    "\n",
    "\n",
    "    # Other\n",
    "    df['NEW_OWN_CAR_AGE_TO_DAYS_BIRTH']=df['OWN_CAR_AGE']/df['DAYS_BIRTH']\n",
    "    df['NEW_OWN_CAR_AGE_TO_DAYS_EMPLOYED']=df['OWN_CAR_AGE']/df['DAYS_EMPLOYED']\n",
    "    df['NEW_DAYS_LAST_PHONE_CHANGE_TO_DAYS_BIRTH']=df['DAYS_LAST_PHONE_CHANGE']/df['DAYS_BIRTH']\n",
    "    df['NEW_DAYS_LAST_PHONE_CHANGE_TO_DAYS_EMPLOYED']=df['DAYS_LAST_PHONE_CHANGE']/df['DAYS_EMPLOYED']\n",
    "    df['NEW_CNT_CHILD_TO_CNT_FAM_MEMBERS']=df['CNT_CHILDREN']/df['CNT_FAM_MEMBERS']\n",
    "    df['NEW_EXT_SOURCES_MEAN']=df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    df['NEW_EXT_SOURCES_STD']=df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "    df['NEW_DAYS_CHANGE_MEAN']=df[['DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE', 'DAYS_REGISTRATION']].mean(axis=1)\n",
    "    df['NEW_REGION_RATING_CLIENT_MEAN']=df[['REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY']].mean(axis=1)\n",
    "    df['NEW_30_CNT_SOCIAL_CIRCLE_MEAN']=df[['OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE']].mean(axis=1)\n",
    "    df['NEW_60_CNT_SOCIAL_CIRCLE_MEAN']=df[['OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE']].mean(axis=1)\n",
    "    \n",
    "    print(f'After adding features: {df.shape}')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def numeric_agg(df, group_col, df_name):\n",
    "    num_df=df.select_dtypes('number')\n",
    "    if num_df.shape[1]!=0:\n",
    "        for c in num_df.columns:\n",
    "            if 'ID' in c and c!=group_col:\n",
    "                num_df=num_df.drop(c, axis=1)\n",
    "        df_agg=num_df.groupby(group_col).agg(['count', 'mean', 'max', 'min', 'sum'])\n",
    "\n",
    "        new_cols=[]\n",
    "        for l1 in df_agg.columns.levels[0]:\n",
    "            if l1!=group_col:\n",
    "                for l2 in df_agg.columns.levels[1]: # for stat in agg.columns.levels[1][:-1]\n",
    "                    new_cols.append(f'{df_name}_{l1}_{l2}')\n",
    "        df_agg.columns=new_cols\n",
    "        # Remove duplicate columns by values\n",
    "        _, idx = np.unique(df_agg, axis = 1, return_index = True)\n",
    "        df_agg = df_agg.iloc[:, idx]\n",
    "        print(f'Dataset:{df_name}\\n\\tBefore: {num_df.shape[1]} numeric cols\\n\\tAfter: {df_agg.shape[1]}')\n",
    "        return df_agg\n",
    "    else:\n",
    "        print('No numeric columns in dataframe')\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "def categ_agg(df, group_col, df_name, enc, enc_mode='train'):\n",
    "    cat_df=df.select_dtypes(include=['category', 'object'])\n",
    "    if cat_df.shape[1]!=0:\n",
    "        if enc_mode=='train':\n",
    "            cat_df_ohe=enc.fit_transform(cat_df)\n",
    "        elif enc_mode=='test': \n",
    "            cat_df_ohe=enc.transform(cat_df)\n",
    "        cat_df_ohe=pd.DataFrame(cat_df_ohe, columns=enc.get_feature_names_out())\n",
    "        cat_df_ohe[group_col]=df[group_col]\n",
    "        df_agg=cat_df_ohe.groupby(group_col).agg(['sum', 'mean'])\n",
    "\n",
    "        new_cols=[]\n",
    "        for l1 in df_agg.columns.levels[0]:\n",
    "            for l2 in ['count', 'count_norm']: # more suitable aliases for sum and mean\n",
    "                new_cols.append(f'{df_name}_{l1}_{l2}')\n",
    "        df_agg.columns=new_cols\n",
    "        # Remove duplicate columns by values\n",
    "        _, idx = np.unique(df_agg, axis = 1, return_index = True)\n",
    "        df_agg = df_agg.iloc[:, idx]\n",
    "        print(f'Dataset:{df_name}\\n\\tBefore: {cat_df.shape[1]} categorical cols\\n\\tAfter: {df_agg.shape[1]}')\n",
    "        return df_agg\n",
    "    else:\n",
    "        print('No categorical columns in dataframe')\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "def agg_combine(df, group_vars, df_names, enc, enc_mode='train', agg_level=1):\n",
    "    if agg_level==2:\n",
    "        df_cat_agg=categ_agg(df, group_vars[1], df_names[1], enc, enc_mode)\n",
    "        df_num_agg=numeric_agg(df, group_vars[1], df_names[1])\n",
    "        df_full_l2=df_cat_agg.merge(df_num_agg, on=group_vars[1], how='outer')\n",
    "        df_full_l2=df[group_vars].merge(df_full_l2, on=group_vars[1], how='right')\n",
    "        df_full=numeric_agg(df_full_l2, group_vars[0], df_names[0])\n",
    "        gc.enable()\n",
    "        del df_full_l2\n",
    "        gc.collect()\n",
    "    elif agg_level==1:\n",
    "        df_cat_agg=categ_agg(df, group_vars[0], df_names[0], enc, enc_mode)\n",
    "        df_num_agg=numeric_agg(df, group_vars[0], df_names[0])\n",
    "        df_full=df_cat_agg.merge(df_num_agg, on=group_vars[0], how='outer')\n",
    "    else:\n",
    "        return 'Select aggregation level 1 or 2'\n",
    "    gc.enable()\n",
    "    del df_cat_agg, df_num_agg\n",
    "    gc.collect()\n",
    "    return df_full\n",
    "\n",
    "\n",
    "\n",
    "def application_data(path):\n",
    "    df=pd.read_csv(path)\n",
    "    print(f'Application data shape: {df.shape}')\n",
    "    df['DAYS_EMPLOYED']=df['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
    "    df=generate_domain_features(df)\n",
    "    return convert_types(df)\n",
    "\n",
    "\n",
    "\n",
    "def bureau_and_bb(bur_path, bb_path, enc_mode='test', bur_ohe=None, bb_ohe=None):\n",
    "    bur=convert_types(pd.read_csv(bur_path))\n",
    "    bb=convert_types(pd.read_csv(bb_path))\n",
    "    print(f'Bureau shape: {bur.shape}')\n",
    "    print(f'Bureau balance shape: {bb.shape}')\n",
    "    if not bb_ohe:\n",
    "        bb_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    bb_numeric_agg_df=numeric_agg(bb, 'SK_ID_BUREAU', 'bureau_balance')\n",
    "    bb_categ_agg_df=categ_agg(bb, 'SK_ID_BUREAU', 'bureau_balance', bb_ohe, enc_mode)\n",
    "    bb_full=bb_numeric_agg_df.merge(bb_categ_agg_df, on='SK_ID_BUREAU', how='outer')\n",
    "    bb_by_credit=bur[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bb_full, on='SK_ID_BUREAU', how='left') \n",
    "    bb=numeric_agg(bb_by_credit, 'SK_ID_CURR', 'loan') \n",
    "    if not bur_ohe:\n",
    "        bur_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    bur=agg_combine(bur, ['SK_ID_CURR'], ['bureau'], bur_ohe, enc_mode)\n",
    "    del bb_numeric_agg_df, bb_categ_agg_df, bb_full, bb_by_credit; gc.collect()\n",
    "    return bur, bb, bur_ohe, bb_ohe\n",
    "\n",
    "\n",
    "\n",
    "def previous(prev_path, enc_mode='test', prev_ohe=None):\n",
    "    prev=convert_types(pd.read_csv(prev_path))\n",
    "    print(f'Previous shape: {prev.shape}')\n",
    "    for c in ['DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION']:\n",
    "        prev[c]=prev[c].replace(365243, np.nan)\n",
    "    if not prev_ohe:\n",
    "        prev_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    prev=agg_combine(prev, ['SK_ID_CURR'], ['previous'], prev_ohe, enc_mode)\n",
    "    return prev, prev_ohe\n",
    "\n",
    "\n",
    "\n",
    "def pos_cash(cash_path, enc_mode='test', cash_ohe=None):\n",
    "    cash=convert_types(pd.read_csv(cash_path))\n",
    "    print(f'Cash shape: {cash.shape}')\n",
    "    if not cash_ohe:\n",
    "        cash_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    cash=agg_combine(cash, ['SK_ID_CURR', 'SK_ID_PREV'], ['loan', 'cash'], cash_ohe, enc_mode, agg_level=2)\n",
    "    return cash, cash_ohe\n",
    "\n",
    "\n",
    "\n",
    "def installments(inst_path):\n",
    "    inst=convert_types(pd.read_csv(inst_path))\n",
    "    print(f'Installments shape: {inst.shape}')\n",
    "    inst_agg_by_prev=numeric_agg(inst, 'SK_ID_PREV', 'inst')\n",
    "    inst_agg_by_prev=inst[['SK_ID_PREV', 'SK_ID_CURR']].merge(inst_agg_by_prev, on='SK_ID_PREV', how='right')\n",
    "    inst=numeric_agg(inst_agg_by_prev, 'SK_ID_CURR', 'loan')\n",
    "    del inst_agg_by_prev; gc.collect()\n",
    "    return inst\n",
    "\n",
    "\n",
    "\n",
    "def card_balance(card_path, enc_mode='test', card_ohe=None):\n",
    "    card_balance=convert_types(pd.read_csv(card_path))\n",
    "    print(f'Card Balance shape: {card_balance.shape}')\n",
    "    if not card_ohe:\n",
    "        card_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    card_balance=agg_combine(card_balance, ['SK_ID_CURR', 'SK_ID_PREV'], ['loan', 'card'], card_ohe, enc_mode, agg_level=2)\n",
    "    return card_balance, card_ohe\n",
    "\n",
    "\n",
    "\n",
    "def full_df(path_dict, mode='train'):\n",
    "    if mode=='train':\n",
    "        ohe_dict={}\n",
    "        app=application_data(path_dict['application_train'])\n",
    "        bur, bb, bur_ohe, bb_ohe=bureau_and_bb(path_dict['bur'], path_dict['bb'])\n",
    "        app=app.merge(bur, on='SK_ID_CURR', how='left')\n",
    "        ohe_dict['Bureau_OHE']=bur_ohe\n",
    "        ohe_dict['BB_OHE']= bb_ohe\n",
    "        del bur, bur_ohe; gc.collect()\n",
    "        app=app.merge(bb, on='SK_ID_CURR', how='left')\n",
    "        del bb, bb_ohe; gc.collect()\n",
    "        \n",
    "        prev, prev_ohe=previous(path_dict['previous'])\n",
    "        app=app.merge(prev, on='SK_ID_CURR', how='left')\n",
    "        ohe_dict['Prev_OHE']= prev_ohe\n",
    "        del prev, prev_ohe; gc.collect()\n",
    "        \n",
    "        cash, cash_ohe=pos_cash(path_dict['cash'])\n",
    "        app=app.merge(cash, on='SK_ID_CURR', how='left')\n",
    "        ohe_dict['Cash_OHE']= cash_ohe\n",
    "        del cash, cash_ohe; gc.collect()\n",
    "        \n",
    "        inst=installments(path_dict['installments'])\n",
    "        app=app.merge(inst, on='SK_ID_CURR', how='left')\n",
    "        del inst; gc.collect()\n",
    "        \n",
    "        card_b, card_b_ohe=card_balance(path_dict['card_balance'])\n",
    "        app=app.merge(card_b, on='SK_ID_CURR', how='left')\n",
    "        ohe_dict['Card_OHE']=card_b_ohe\n",
    "        del card_b, card_b_ohe; gc.collect()\n",
    "       \n",
    "        with open(path_dict['ohe_dict'], 'wb') as f:\n",
    "            pickle.dump(ohe_dict, f)\n",
    "        \n",
    "    elif mode=='test':\n",
    "        with open(path_dict['ohe_dict'], 'rb') as f:\n",
    "            ohe_dict=pickle.load(f)\n",
    "        app=application_data(path_dict['application_test'])\n",
    "        bur, bb, _, _=bureau_and_bb(path_dict['bur'], \n",
    "                                    path_dict['bb'], \n",
    "                                    bur_ohe=ohe_dict['Bureau_OHE'], \n",
    "                                    bb_ohe=ohe_dict['BB_OHE'])\n",
    "        app=app.merge(bur, on='SK_ID_CURR', how='left')\n",
    "        del bur; gc.collect()\n",
    "        app=app.merge(bb, on='SK_ID_CURR', how='left')\n",
    "        del bb; gc.collect()\n",
    "        \n",
    "        prev, _=previous(path_dict['previous'], prev_ohe=ohe_dict['Prev_OHE'])\n",
    "        app=app.merge(prev, on='SK_ID_CURR', how='left')\n",
    "        del prev; gc.collect()\n",
    "        \n",
    "        cash, _=pos_cash(path_dict['cash'], cash_ohe=ohe_dict['Cash_OHE'])\n",
    "        app=app.merge(cash, on='SK_ID_CURR', how='left')\n",
    "        del cash; gc.collect()\n",
    "        \n",
    "        inst=installments(path_dict['installments'])\n",
    "        app=app.merge(inst, on='SK_ID_CURR', how='left')\n",
    "        del inst; gc.collect()\n",
    "        \n",
    "        card_b, _=card_balance(path_dict['card_balance'], card_ohe=ohe_dict['Card_OHE'])\n",
    "        app=app.merge(card_b, on='SK_ID_CURR', how='left')\n",
    "        del card_b; gc.collect()\n",
    "    else:\n",
    "        raise Exception(\"Specify mode (train of test) and OHE dict bool\")\n",
    "\n",
    "    app=app.rename(columns=lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    return convert_types(app)\n",
    "\n",
    "\n",
    "\n",
    "def correlation_filter(df, thresh, corr_path, mode='train'):\n",
    "    if mode=='train':\n",
    "        corr_mat=df.drop(['SK_ID_CURR', 'TARGET'], axis=1).corr().abs()\n",
    "        upper=corr_mat.where(np.triu(np.ones(corr_mat.shape), k=1).astype(np.bool))\n",
    "        upper.to_csv(corr_path, index=False)\n",
    "    elif mode=='test':\n",
    "        upper=pd.read_csv(corr_path)\n",
    "    to_drop=[column for column in upper.columns if any(upper[column]>thresh)]\n",
    "    print(f'Correlation: {len(to_drop)} will be removed')\n",
    "    return df.drop(to_drop, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def missing_filter(df, thresh, col_path, mode='train'):\n",
    "    if mode=='train':\n",
    "        mt=miss_table(df)\n",
    "        to_drop=mt.loc[mt['Percent']>thresh].index\n",
    "        with open(col_path, 'wb') as f:\n",
    "            pickle.dump(to_drop, f)\n",
    "    elif mode=='test':\n",
    "        with open(col_path, 'rb') as f:\n",
    "            to_drop=pickle.load(f)\n",
    "    else:\n",
    "        raise Exception(\"Specify mode (train of test) and path\")\n",
    "    print(f'{len(to_drop)} features with {thresh}% of NaNs will be removed')\n",
    "    return df.drop(to_drop, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def zero_var_filter(df, col_path, mode='train'):\n",
    "    if mode=='train':\n",
    "        scaler=MinMaxScaler()\n",
    "        numeric_data=df.drop('SK_ID_CURR', axis=1).select_dtypes('number').reset_index(drop=True)\n",
    "        numeric_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        for i in numeric_data.columns:\n",
    "            numeric_data[i].fillna(value=numeric_data[i].mean(), inplace=True) #replace NaN with mean of dimension\n",
    "            numeric_data[i]=scaler.fit_transform(numeric_data[i].values.reshape(-1,1)) \n",
    "        vars_df=numeric_data.var()\n",
    "        to_drop=vars_df[vars_df==0].index\n",
    "        with open(col_path, 'wb') as f:\n",
    "            pickle.dump(to_drop, f)\n",
    "    \n",
    "    elif mode=='test':\n",
    "        with open(col_path, 'rb') as f:\n",
    "            to_drop=pickle.load(f)\n",
    "    else:\n",
    "        raise Exception(\"Specify mode (train of test) and path\")\n",
    "    print(f'{len(to_drop)} features with zero variance will be removed')\n",
    "    return df.drop(to_drop, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def feat_imp_cv(df, feat_imp_path, k=5, params=None, mode='train'):\n",
    "    if mode=='train':\n",
    "        for c in df:\n",
    "            if (df[c].dtype=='object') and (df[c].nunique()<df.shape[0]):\n",
    "                df[c]=df[c].astype('category')\n",
    "        X, y=df.drop(['SK_ID_CURR', 'TARGET'], axis=1), df['TARGET']\n",
    "        feat_importances_gain, feat_importances_split=[], []\n",
    "        kfold=StratifiedKFold(k)\n",
    "        for f, (tr, te) in enumerate(kfold.split(X, y=y)):\n",
    "            X_train, y_train=X.iloc[tr, :], y.iloc[tr]\n",
    "            X_test, y_test=X.iloc[te, :], y.iloc[te]\n",
    "            weight=np.count_nonzero(y_train==0)/np.count_nonzero(y_train==1)\n",
    "            params['scale_pos_weight']=weight\n",
    "            dtrain=lgb.Dataset(X_train, label=y_train, params={'verbose': -1})\n",
    "            dval=lgb.Dataset(X_test, label=y_test, params={'verbose': -1})\n",
    "            model=lgb.train(\n",
    "                            params=params,\n",
    "                            train_set=dtrain,\n",
    "                            valid_sets=[dtrain, dval],\n",
    "                            valid_names=['train', 'test'],\n",
    "                            categorical_feature=list(X.select_dtypes(['category']).columns),\n",
    "                            callbacks=[lgb.early_stopping(100, verbose=-1)],\n",
    "                            verbose_eval=False\n",
    "                            )\n",
    "            feat_importances_gain.append(model.feature_importance(importance_type='gain'))\n",
    "            feat_importances_split.append(model.feature_importance(importance_type='split'))\n",
    "            \n",
    "        feat_importances_gain=np.array(feat_importances_gain).mean(axis=0)\n",
    "        feat_importances_split=np.array(feat_importances_split).mean(axis=0)\n",
    "        feat_importances_df=pd.DataFrame({'feature': list(X.columns),\n",
    "                                        'importance (gain)': feat_importances_gain,\n",
    "                                        'importance (split)': feat_importances_split,})\n",
    "        with open(feat_imp_path, 'wb') as f:\n",
    "            pickle.dump(feat_importances_df, f)\n",
    "    \n",
    "    elif mode=='test':\n",
    "        with open(feat_imp_path, 'rb') as f:\n",
    "            feat_importances_df=pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"Specify mode (train of test) and path\")\n",
    "    return feat_importances_df\n",
    "\n",
    "\n",
    "\n",
    "def drop_zero_imp(df, feat_imp_path, k=5, params=None, mode='train', drop_by='importance (gain)'):\n",
    "    feature_imp_df=feat_imp_cv(df, feat_imp_path, k, params, mode=mode)\n",
    "    to_drop=feature_imp_df[feature_imp_df[drop_by]==0]['feature'].values\n",
    "    print(f'Num of features with zero importance: {len(to_drop)}')\n",
    "    return df.drop(to_drop, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def save_data(df, df_path, dtypes_path=None):\n",
    "    df.to_csv(df_path, index=False)\n",
    "    if dtypes_path:\n",
    "        with open(dtypes_path, 'wb') as f:\n",
    "            pickle.dump(df.dtypes, f)\n",
    "    print(f'Size: {round(sys.getsizeof(df) / 1e9, 2)}gb\\nShape: {df.shape}\\nSaved to: {df_path}')\n",
    "\n",
    "\n",
    "\n",
    "def train_model(df, params, model_path, col_tran_path):\n",
    "    X, y=df.drop(['SK_ID_CURR', 'TARGET'], axis=1), df['TARGET']\n",
    "    cat_cols, num_cols=X.select_dtypes(include=['category', 'object']).columns, X.select_dtypes('number').columns\n",
    "    ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "    col_tran=ColumnTransformer([\n",
    "        ('cat', ohe, cat_cols),\n",
    "        ('num', 'passthrough', num_cols)\n",
    "    ])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=52, stratify=y)\n",
    "    X_train=col_tran.fit_transform(X_train)\n",
    "    X_val=col_tran.transform(X_val)\n",
    "    with open(col_tran_path, 'wb') as f:\n",
    "            pickle.dump(col_tran, f)\n",
    "    dtrain=lgb.Dataset(X_train, label=y_train, params={'verbose': -1})\n",
    "    dval=lgb.Dataset(X_val, label=y_val, params={'verbose': -1})\n",
    "    weight=np.count_nonzero(y==0)/np.count_nonzero(y==1)\n",
    "    params['scale_pos_weight']=weight\n",
    "    model=lgb.train(\n",
    "                    params=params,\n",
    "                    train_set=dtrain,\n",
    "                    valid_sets=[dtrain, dval],\n",
    "                    valid_names=['train', 'test'],\n",
    "                    callbacks=[lgb.early_stopping(100, verbose=-1)],\n",
    "                    verbose_eval=False\n",
    "                    )\n",
    "    model.save_model(model_path)\n",
    "    \n",
    "    return f'Model is saved in {model_path}'\n",
    "\n",
    "\n",
    "\n",
    "def make_prediction(X, model_path, col_tran_path, save_path=None):\n",
    "    with open(col_tran_path, 'rb') as f:\n",
    "        col_tran=pickle.load(f)\n",
    "    model=lgb.Booster(model_file=model_path)\n",
    "    pred=model.predict(col_tran.transform(X.drop('SK_ID_CURR', axis=1)))\n",
    "    if save_path:\n",
    "        submit=X[['SK_ID_CURR']]\n",
    "        submit.loc[:, 'TARGET']=pred\n",
    "        submit.to_csv(save_path, index=False)\n",
    "        return submit\n",
    "    return pred\n",
    "\n",
    "\n",
    "\n",
    "def read_sample(sample_path, nrows=None):\n",
    "    s=pd.read_csv(sample_path, nrows=nrows).to_json(orient='records')\n",
    "    s=json.loads(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52535db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import config\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "def timer():\n",
    "    print(f'Current time: {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "def full_train():\n",
    "    path_dict=config.PATH_DICT\n",
    "    params=config.PARAMS\n",
    "    print('TRAIN PIPELINE')\n",
    "    timer()\n",
    "    print('\\n*** 1. Feature Generation ***')\n",
    "    train=full_df(path_dict, \n",
    "                mode='train')\n",
    "    timer()\n",
    "    print('\\n*** 2. Correlation ***')\n",
    "    train=correlation_filter(train, \n",
    "                            thresh=0.9, \n",
    "                            corr_path=path_dict['corr_matrix'], \n",
    "                            mode='train')\n",
    "    timer()\n",
    "    print('\\n*** 3. Missing Variables ***')\n",
    "    train=missing_filter(train, \n",
    "                        thresh=80, \n",
    "                        col_path=path_dict['missing_columns_drop'], \n",
    "                        mode='train')\n",
    "    timer()\n",
    "    print('\\n*** 4. Zero Variance ***')\n",
    "    train=zero_var_filter(train, \n",
    "                        col_path=path_dict['zero_variance_drop'], \n",
    "                        mode='train')\n",
    "    timer()\n",
    "    print('\\n*** 5. Zero Importance ***')\n",
    "    train=drop_zero_imp(train, \n",
    "                        feat_imp_path=path_dict['zero_imp_drop'], \n",
    "                        k=5, \n",
    "                        params=params, \n",
    "                        mode='train', \n",
    "                        drop_by='importance (gain)')\n",
    "    timer()\n",
    "    save_data(train, path_dict['train_ready_file'], path_dict['dtypes'])\n",
    "    print('\\n*** 6. Model Training ***')\n",
    "    train_model(train, \n",
    "                params=params, \n",
    "                model_path=path_dict['model_file'],\n",
    "                col_tran_path=path_dict['lgb_ohe'])\n",
    "    \n",
    "    print(f'\\n*** DONE : {(time.time()-start_time)/60:.3f} Minutes ***')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    full_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d730ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "\n",
    "from utils import *\n",
    "import config\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "def timer():\n",
    "    print(f'Current time: {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "def full_test():\n",
    "    path_dict=config.PATH_DICT\n",
    "    print('TEST PIPELINE')\n",
    "    timer()\n",
    "    print('\\n*** 1. Feature Generation ***')\n",
    "    test=full_df(path_dict, \n",
    "                mode='test')\n",
    "    timer()\n",
    "    print('\\n*** 2. Correlation ***')\n",
    "    test=correlation_filter(test, \n",
    "                            thresh=0.9, \n",
    "                            corr_path=path_dict['corr_matrix'], \n",
    "                            mode='test')\n",
    "    timer()\n",
    "    print('\\n*** 3. Missing Variables ***')\n",
    "    test=missing_filter(test, \n",
    "                        thresh=80, \n",
    "                        col_path=path_dict['missing_columns_drop'], \n",
    "                        mode='test')\n",
    "    timer()\n",
    "    print('\\n*** 4. Zero Variance ***')\n",
    "    test=zero_var_filter(test, \n",
    "                        col_path=path_dict['zero_variance_drop'], \n",
    "                        mode='test')\n",
    "    timer()\n",
    "    print('\\n*** 5. Zero Importance ***')\n",
    "    test=drop_zero_imp(test, \n",
    "                        feat_imp_path=path_dict['zero_imp_drop'], \n",
    "                        k=5, \n",
    "                        params=None, \n",
    "                        mode='test', \n",
    "                        drop_by='importance (gain)')\n",
    "    timer()\n",
    "    save_data(test, path_dict['test_ready_file'])\n",
    "    print('\\n*** 6. Making Prediction ***')\n",
    "    make_prediction(test, \n",
    "                    model_path=path_dict['model_file'],\n",
    "                    col_tran_path=path_dict['lgb_ohe'],\n",
    "                    save_path=path_dict['submit'])\n",
    "        \n",
    "    print(f'\\n*** DONE : {(time.time()-start_time)/60:.2f} Minutes ***')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    full_test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db383f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "\n",
    "PATH_DICT={\n",
    "    'application_train': '../data/application_train.csv',\n",
    "    'application_test': '../data/application_test.csv',\n",
    "    'bur': '../data/bureau.csv',\n",
    "    'bb': '../data/bureau_balance.csv',\n",
    "    'previous': '../data/previous_application.csv',\n",
    "    'cash': '../data/POS_CASH_balance.csv',\n",
    "    'installments': '../data/installments_payments.csv',\n",
    "    'card_balance': '../data/credit_card_balance.csv',\n",
    "    'ohe_dict': '../models/pipeline/ohe_dict.pkl',\n",
    "    'corr_matrix': '../data/corr_matrix.csv',\n",
    "    'missing_columns_drop': '../models/pipeline/missing_columns_drop.pkl',\n",
    "    'zero_variance_drop': '../models/pipeline/zero_var_columns_drop.pkl',\n",
    "    'zero_imp_drop': '../models/pipeline/zero_imp.pkl',\n",
    "    'model_file': '../models/model.txt',\n",
    "    'lgb_ohe': '../models/pipeline/lgb_ohe.pkl',\n",
    "    'train_ready_file': '../data/train_ready.csv',\n",
    "    'test_ready_file': '../data/test_ready.csv',\n",
    "    'dtypes': '../models/pipeline/dtypes.pkl',\n",
    "    'submit': '../data/submit.csv',\n",
    "    }\n",
    "\n",
    "\n",
    "PARAMS={\n",
    "    'num_boost_round': 10000,\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.05,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 5,\n",
    "    'verbose': -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1464159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend.py\n",
    "\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "import sys\n",
    "sys.path.insert(1, '../src')\n",
    "import config\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "\n",
    "path_dict=config.PATH_DICT\n",
    "\n",
    "app = FastAPI()\n",
    " \n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {'HII': 'WORLD'}\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def post_prediction(data):\n",
    "    prediction=predict_api(data, \n",
    "                            path_dict['dtypes'], \n",
    "                            path_dict['model_file'],\n",
    "                            path_dict['lgb_ohe'])\n",
    "    return {'prob': prediction,\n",
    "            'default': 0 if prediction<0.5 else 1}\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    uvicorn.run('backend:app', host='0.0.0.0', port=8000, reload=True)\n",
    "\n",
    "# python -m uvicorn main:app --reload  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2396ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import sys\n",
    "sys.path.insert(1, '../src')\n",
    "import config\n",
    "import requests\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "st.title('Credit Risk Modelling')\n",
    "uploaded_file=st.file_uploader(\"Choose a file\")\n",
    "if uploaded_file is not None:\n",
    "    df=pd.read_csv(uploaded_file)\n",
    "    data=read_sample(df)\n",
    "    # st.write(data)\n",
    "    response=requests.post('http://127.0.0.1:8000/predict', json=data)\n",
    "    pred=response.text\n",
    "    st.success(f'{pred}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
