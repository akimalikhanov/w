{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1b54d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63846667 0.35799425 0.58598419 0.55709848]\n",
      "[[3]\n",
      " [1]\n",
      " [3]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_predictions(probas):\n",
    "    return np.mean(probas, axis=1)\n",
    "\n",
    "def max_voting(preds):\n",
    "    idxs=np.argmax(preds, axis=1)\n",
    "    return np.take_along_axis(preds, idxs[:, None], axis=1)\n",
    "\n",
    "probs=np.random.rand(4,5)\n",
    "preds=np.random.randint(0,4,(4,5))\n",
    "\n",
    "print(mean_predictions(probs))\n",
    "print(max_voting(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85ffe9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4595349  0.5283434  0.83220901 0.2237004  0.8352632 ]\n",
      " [0.85293325 0.18130427 0.3596681  0.50345398 0.70738105]\n",
      " [0.22695863 0.26101784 0.85074546 0.27082399 0.46517596]\n",
      " [0.23093361 0.80933674 0.64141253 0.8078626  0.85931651]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.6, 2.2, 2. , 3.2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def rank_mean(probas):\n",
    "    ranked=[]\n",
    "    for i in range(probas.shape[1]):\n",
    "        rank_data=stats.rankdata(probas[:, i])\n",
    "        ranked.append(rank_data)\n",
    "    ranked=np.column_stack(ranked)\n",
    "    return np.mean(ranked, axis=1)\n",
    "\n",
    "probs=np.random.rand(4,5)\n",
    "print(probs)\n",
    "rank_mean(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b986e588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [3. 4. 1. 2.]\n",
      "1 [3. 1. 2. 4.]\n",
      "2 [3. 1. 4. 2.]\n",
      "3 [1. 3. 2. 4.]\n",
      "4 [3. 2. 1. 4.]\n",
      "[[3. 4. 1. 2.]\n",
      " [3. 1. 2. 4.]\n",
      " [3. 1. 4. 2.]\n",
      " [1. 3. 2. 4.]\n",
      " [3. 2. 1. 4.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.5, 2.5, 2.5, 2.5, 2.5])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankedd=[]\n",
    "for i in range(probs.shape[1]):\n",
    "    rank_data=stats.rankdata(probs[:, i])\n",
    "    rankedd.append(rank_data)\n",
    "    print(i, rank_data)\n",
    "    ranked=np.column_stack(ranked)\n",
    "\n",
    "print(ranked)\n",
    "np.mean(ranked, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a9d5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from scipy.optimize import fmin\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "class OptimizeAUC:\n",
    "    def __init__(self):\n",
    "        self.coef_=0\n",
    "        \n",
    "    def _auc(self, coef, X, y):\n",
    "        x_coef=X*coef\n",
    "        preds=np.sum(x_coef, axis=1)\n",
    "        auc_score = metrics.roc_auc_score(y, preds)\n",
    "        return -1.0*auc_score\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        loss_partial=partial(self._auc, X=X, y=y)\n",
    "        initial_coef=np.random.dirichlet(np.ones(X.shape[1]), size=1)\n",
    "        self.coef_=fmin(loss_partial, initial_coef, disp=True)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        x_coef=X*self.coef_\n",
    "        preds=np.sum(x_coef, axis=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ae01d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:07:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold-2: LR AUC = 0.9643058971557743\n",
      "Fold-2: RF AUC = 0.98520839053337\n",
      "Fold-2: XGB AUC = 0.9856955108451269\n",
      "Fold-2: Average Pred AUC = 0.9834028693778364\n",
      "[11:07:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold-1: LR AUC = 0.9610986951031649\n",
      "Fold-1: RF AUC = 0.9858444709404615\n",
      "Fold-1: XGB AUC = 0.9878575922288588\n",
      "Fold-1: Average Pred AUC = 0.9841047898270654\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.514162\n",
      "         Iterations: 56\n",
      "         Function evaluations: 114\n",
      "Optimized AUC, Fold 2 = 0.9864641513370569\n",
      "Coefficients = [-0.05289877  0.35647758  0.83209972]\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.986797\n",
      "         Iterations: 42\n",
      "         Function evaluations: 92\n",
      "Optimized AUC, Fold 1 = 0.9878161522023374\n",
      "Coefficients = [-0.01362855  0.21467562  1.01087033]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import ensemble, linear_model, metrics, model_selection\n",
    "\n",
    "X,y=make_classification(n_samples=10000, n_features=25)\n",
    "xfold1, xfold2, yfold1, yfold2=model_selection.train_test_split(X, y, test_size=0.5, stratify=y)\n",
    "\n",
    "\n",
    "# fit on fold1, pred on fold2\n",
    "logreg=linear_model.LogisticRegression()\n",
    "rf=ensemble.RandomForestClassifier()\n",
    "xgb_clf=xgb.XGBClassifier(use_label_encoder=False)\n",
    "\n",
    "logreg.fit(xfold1, yfold1)\n",
    "rf.fit(xfold1, yfold1)\n",
    "xgb_clf.fit(xfold1, yfold1)\n",
    "\n",
    "pred_logreg, pred_rf, pred_xgb=logreg.predict_proba(xfold2)[:, 1], rf.predict_proba(xfold2)[:, 1], xgb_clf.predict_proba(xfold2)[:, 1]\n",
    "avg_pred=(pred_logreg+pred_rf+pred_xgb)/3\n",
    "\n",
    "fold2_preds=np.column_stack((pred_logreg, pred_rf, pred_xgb, avg_pred))\n",
    "\n",
    "aucs_fold2=[]\n",
    "for i in range(fold2_preds.shape[1]):\n",
    "    auc=metrics.roc_auc_score(yfold2, fold2_preds[:, i])\n",
    "    aucs_fold2.append(auc)\n",
    "    \n",
    "print(f\"Fold-2: LR AUC = {aucs_fold2[0]}\")\n",
    "print(f\"Fold-2: RF AUC = {aucs_fold2[1]}\")\n",
    "print(f\"Fold-2: XGB AUC = {aucs_fold2[2]}\")\n",
    "print(f\"Fold-2: Average Pred AUC = {aucs_fold2[3]}\")\n",
    "\n",
    "\n",
    "# fit on fold2, pred on fold1\n",
    "logreg=linear_model.LogisticRegression()\n",
    "rf=ensemble.RandomForestClassifier()\n",
    "xgb_clf=xgb.XGBClassifier(use_label_encoder=False)\n",
    "\n",
    "logreg.fit(xfold2, yfold2)\n",
    "rf.fit(xfold2, yfold2)\n",
    "xgb_clf.fit(xfold2, yfold2)\n",
    "\n",
    "pred_logreg, pred_rf, pred_xgb=logreg.predict_proba(xfold1)[:, 1], rf.predict_proba(xfold1)[:, 1], xgb_clf.predict_proba(xfold1)[:, 1]\n",
    "avg_pred=(pred_logreg+pred_rf+pred_xgb)/3\n",
    "\n",
    "fold1_preds=np.column_stack((pred_logreg, pred_rf, pred_xgb, avg_pred))\n",
    "\n",
    "aucs_fold1=[]\n",
    "for i in range(fold1_preds.shape[1]):\n",
    "    auc=metrics.roc_auc_score(yfold1, fold1_preds[:, i])\n",
    "    aucs_fold1.append(auc)\n",
    "    \n",
    "print(f\"Fold-1: LR AUC = {aucs_fold1[0]}\")\n",
    "print(f\"Fold-1: RF AUC = {aucs_fold1[1]}\")\n",
    "print(f\"Fold-1: XGB AUC = {aucs_fold1[2]}\")\n",
    "print(f\"Fold-1: Average Pred AUC = {aucs_fold1[3]}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "opt=OptimizeAUC()\n",
    "opt.fit(fold1_preds[:, :-1], yfold2)\n",
    "opt_preds_fold2=opt.predict(fold2_preds[:, :-1])\n",
    "auc=metrics.roc_auc_score(yfold2, opt_preds_fold2)\n",
    "print(f\"Optimized AUC, Fold 2 = {auc}\")\n",
    "print(f\"Coefficients = {opt.coef_}\\n\")\n",
    "\n",
    "opt=OptimizeAUC()\n",
    "opt.fit(fold2_preds[:, :-1], yfold2)\n",
    "opt_preds_fold1=opt.predict(fold1_preds[:, :-1])\n",
    "auc=metrics.roc_auc_score(yfold1, opt_preds_fold1)\n",
    "print(f\"Optimized AUC, Fold 1 = {auc}\")\n",
    "print(f\"Coefficients = {opt.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d42d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
