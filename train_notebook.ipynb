{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a35b1694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyod.models.mad import MAD\n",
    "from scipy.stats import normaltest\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "sns.set()\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "from imports import *\n",
    "import gc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609ccfc1",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3480f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_data(path):\n",
    "    df=pd.read_csv(path)\n",
    "    df['DAYS_EMPLOYED']=df['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
    "    df=generate_domain_features(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def bureau_and_bb(bur_path, bb_path, enc_mode='test', bur_ohe=None, bb_ohe=None):\n",
    "    bur=convert_types(pd.read_csv(bur_path))\n",
    "    bb=convert_types(pd.read_csv(bb_path))\n",
    "    print(f'Bureau shape before: {bur.shape}')\n",
    "    print(f'Bureau balance shape before: {bb.shape}')\n",
    "    if not bb_ohe:\n",
    "        bb_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    bb_numeric_agg_df=numeric_agg(bb, 'SK_ID_BUREAU', 'bureau_balance')\n",
    "    bb_categ_agg_df=categ_agg(bb, 'SK_ID_BUREAU', 'bureau_balance', bb_ohe, enc_mode)\n",
    "    bb_full=bb_numeric_agg_df.merge(bb_categ_agg_df, on='SK_ID_BUREAU', how='outer')\n",
    "    bb_by_credit=bur[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bb_full, on='SK_ID_BUREAU', how='left') \n",
    "    bb=numeric_agg(bb_by_credit, 'SK_ID_CURR', 'loan') \n",
    "    if not bur_ohe:\n",
    "        bur_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    bur=agg_combine(bur, ['SK_ID_CURR'], ['bureau'], bur_ohe, enc_mode)\n",
    "    del bb_numeric_agg_df, bb_categ_agg_df, bb_full, bb_by_credit; gc.collect()\n",
    "    return bur, bb, bur_ohe, bb_ohe\n",
    "\n",
    "\n",
    "def previous(prev_path, enc_mode='test', prev_ohe=None):\n",
    "    prev=convert_types(pd.read_csv(prev_path))\n",
    "    for c in ['DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION']:\n",
    "        prev[c]=prev[c].replace(365243, np.nan)\n",
    "    if not prev_ohe:\n",
    "        prev_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    prev=agg_combine(prev, ['SK_ID_CURR'], ['previous'], prev_ohe, enc_mode)\n",
    "    return prev, prev_ohe\n",
    "\n",
    "\n",
    "def pos_cash(cash_path, enc_mode='test', cash_ohe=None):\n",
    "    cash=convert_types(pd.read_csv(cash_path))\n",
    "    if not cash_ohe:\n",
    "        cash_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    cash=agg_combine(cash, ['SK_ID_CURR', 'SK_ID_PREV'], ['loan', 'cash'], cash_ohe, enc_mode, agg_level=2)\n",
    "    return cash, cash_ohe\n",
    "\n",
    "\n",
    "def installments(inst_path):\n",
    "    inst=convert_types(pd.read_csv(inst_path))\n",
    "    inst_agg_by_prev=numeric_agg(inst, 'SK_ID_PREV', 'inst')\n",
    "    inst_agg_by_prev=inst[['SK_ID_PREV', 'SK_ID_CURR']].merge(inst_agg_by_prev, on='SK_ID_PREV', how='right')\n",
    "    inst=numeric_agg(inst_agg_by_prev, 'SK_ID_CURR', 'loan')\n",
    "    del inst_agg_by_prev; gc.collect()\n",
    "    return inst\n",
    "\n",
    "\n",
    "def card_balance(card_path, enc_mode='test', card_ohe=None):\n",
    "    card_balance=convert_types(pd.read_csv(card_path))\n",
    "    if not card_ohe:\n",
    "        card_ohe=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        enc_mode='train'\n",
    "    card_balance=agg_combine(card_balance, ['SK_ID_CURR', 'SK_ID_PREV'], ['loan', 'card'], card_ohe, enc_mode, agg_level=2)\n",
    "    return card_balance, card_ohe\n",
    "\n",
    "\n",
    "path_dict={\n",
    "    'application_train': '../data/application_train.csv',\n",
    "    'application_test': '../data/application_test.csv',\n",
    "    'bur': '../data/bureau.csv',\n",
    "    'bb': '../data/bureau_balance.csv',\n",
    "    'previous': '../data/previous_application.csv',\n",
    "    'cash': '../data/POS_CASH_balance.csv',\n",
    "    'installments': '../data/installments_payments.csv',\n",
    "    'card_balance': '../data/credit_card_balance.csv',\n",
    "    'ohe_dict': '../data/ohe_dict.pkl',\n",
    "}\n",
    "\n",
    "\n",
    "def full_df(path_dict, ohe_dict_bool, mode='train'):\n",
    "    if mode=='train' and not ohe_dict_bool:\n",
    "        ohe_dict={}\n",
    "        app=application_data(path_dict['application_train'])\n",
    "        bur, bb, bur_ohe, bb_ohe=bureau_and_bb(path_dict['bur'], path_dict['bb'])\n",
    "        app=app.merge(bur, on='SK_ID_CURR', how='left')\n",
    "        ohe_dict['Bureau_OHE']=bur_ohe\n",
    "        ohe_dict['BB_OHE']= bb_ohe\n",
    "        del bur, bur_ohe; gc.collect()\n",
    "        app=app.merge(bb, on='SK_ID_CURR', how='left')\n",
    "        del bb, bb_ohe; gc.collect()\n",
    "        \n",
    "        prev, prev_ohe=previous(path_dict['previous'])\n",
    "        app=app.merge(prev, on='SK_ID_CURR', how='left')\n",
    "        ohe_dict['Prev_OHE']= prev_ohe\n",
    "        del prev, prev_ohe; gc.collect()\n",
    "        \n",
    "        cash, cash_ohe=pos_cash(path_dict['cash'])\n",
    "        app=app.merge(cash, on='SK_ID_CURR', how='left')\n",
    "        ohe_dict['Cash_OHE']= cash_ohe\n",
    "        del cash, cash_ohe; gc.collect()\n",
    "        \n",
    "        inst=installments(path_dict['installments'])\n",
    "        app=app.merge(inst, on='SK_ID_CURR', how='left')\n",
    "        del inst; gc.collect()\n",
    "        \n",
    "        card_b, card_b_ohe=card_balance(path_dict['card_balance'])\n",
    "        app=app.merge(card_b, on='SK_ID_CURR', how='left')\n",
    "        ohe_dict['Card_OHE']=card_b_ohe\n",
    "        del card_b, card_b_ohe; gc.collect()\n",
    "       \n",
    "        with open(path_dict['ohe_dict'], 'wb') as f:\n",
    "            pickle.dump(ohe_dict, f)\n",
    "        \n",
    "    elif mode=='test' and ohe_dict_bool:\n",
    "        with open(path_dict['ohe_dict'], 'rb') as f:\n",
    "            ohe_dict=pickle.load(f)\n",
    "        app=application_data(path_dict['application_test'])\n",
    "        bur, bb, _, _=bureau_and_bb(path_dict['bur'], \n",
    "                                    path_dict['bb'], \n",
    "                                    bur_ohe=ohe_dict['Bureau_OHE'], \n",
    "                                    bb_ohe=ohe_dict['BB_OHE'])\n",
    "        app=app.merge(bur, on='SK_ID_CURR', how='left')\n",
    "        del bur; gc.collect()\n",
    "        app=app.merge(bb, on='SK_ID_CURR', how='left')\n",
    "        del bb; gc.collect()\n",
    "        \n",
    "        prev, _=previous(path_dict['previous'], prev_ohe=ohe_dict['Prev_OHE'])\n",
    "        app=app.merge(prev, on='SK_ID_CURR', how='left')\n",
    "        del prev; gc.collect()\n",
    "        \n",
    "        cash, _=pos_cash(path_dict['cash'], cash_ohe=ohe_dict['Cash_OHE'])\n",
    "        app=app.merge(cash, on='SK_ID_CURR', how='left')\n",
    "        del cash; gc.collect()\n",
    "        \n",
    "        inst=installments(path_dict['installments'])\n",
    "        app=app.merge(inst, on='SK_ID_CURR', how='left')\n",
    "        del inst; gc.collect()\n",
    "        \n",
    "        card_b, _=card_balance(path_dict['card_balance'], card_ohe=ohe_dict['Card_OHE'])\n",
    "        app=app.merge(card_b, on='SK_ID_CURR', how='left')\n",
    "        del card_b; gc.collect()\n",
    "    else:\n",
    "        raise Exception(\"Specify mode (train of test) and OHE dict bool\")\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f78889f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding features: (307511, 122)\n",
      "After adding features: (307511, 166)\n",
      "Bureau shape before: (1716428, 17)\n",
      "Bureau balance shape before: (27299925, 3)\n",
      "Dataset:bureau_balance\n",
      "\tBefore: 2 numeric cols\n",
      "\tAfter: 5\n",
      "Dataset:bureau_balance\n",
      "\tBefore: 1 categorical cols\n",
      "\tAfter: 16\n",
      "Dataset:loan\n",
      "\tBefore: 22 numeric cols\n",
      "\tAfter: 85\n",
      "Dataset:bureau\n",
      "\tBefore: 3 categorical cols\n",
      "\tAfter: 46\n",
      "Dataset:bureau\n",
      "\tBefore: 13 numeric cols\n",
      "\tAfter: 56\n",
      "Dataset:previous\n",
      "\tBefore: 16 categorical cols\n",
      "\tAfter: 286\n",
      "Dataset:previous\n",
      "\tBefore: 19 numeric cols\n",
      "\tAfter: 85\n",
      "Dataset:cash\n",
      "\tBefore: 1 categorical cols\n",
      "\tAfter: 18\n",
      "Dataset:cash\n",
      "\tBefore: 6 numeric cols\n",
      "\tAfter: 23\n",
      "Dataset:loan\n",
      "\tBefore: 42 numeric cols\n",
      "\tAfter: 162\n",
      "Dataset:inst\n",
      "\tBefore: 7 numeric cols\n",
      "\tAfter: 26\n",
      "Dataset:loan\n",
      "\tBefore: 27 numeric cols\n",
      "\tAfter: 106\n",
      "Dataset:card\n",
      "\tBefore: 1 categorical cols\n",
      "\tAfter: 14\n",
      "Dataset:card\n",
      "\tBefore: 21 numeric cols\n",
      "\tAfter: 83\n",
      "Dataset:loan\n",
      "\tBefore: 98 numeric cols\n",
      "\tAfter: 376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(307511, 1368)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=full_df(path_dict, False, mode='train')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21143824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding features: (48744, 121)\n",
      "After adding features: (48744, 165)\n",
      "Bureau shape before: (1716428, 17)\n",
      "Bureau balance shape before: (27299925, 3)\n",
      "Dataset:bureau_balance\n",
      "\tBefore: 2 numeric cols\n",
      "\tAfter: 5\n",
      "Dataset:bureau_balance\n",
      "\tBefore: 1 categorical cols\n",
      "\tAfter: 16\n",
      "Dataset:loan\n",
      "\tBefore: 22 numeric cols\n",
      "\tAfter: 85\n",
      "Dataset:bureau\n",
      "\tBefore: 3 categorical cols\n",
      "\tAfter: 46\n",
      "Dataset:bureau\n",
      "\tBefore: 13 numeric cols\n",
      "\tAfter: 56\n",
      "Dataset:previous\n",
      "\tBefore: 16 categorical cols\n",
      "\tAfter: 286\n",
      "Dataset:previous\n",
      "\tBefore: 19 numeric cols\n",
      "\tAfter: 85\n",
      "Dataset:cash\n",
      "\tBefore: 1 categorical cols\n",
      "\tAfter: 18\n",
      "Dataset:cash\n",
      "\tBefore: 6 numeric cols\n",
      "\tAfter: 23\n",
      "Dataset:loan\n",
      "\tBefore: 42 numeric cols\n",
      "\tAfter: 162\n",
      "Dataset:inst\n",
      "\tBefore: 7 numeric cols\n",
      "\tAfter: 26\n",
      "Dataset:loan\n",
      "\tBefore: 27 numeric cols\n",
      "\tAfter: 106\n",
      "Dataset:card\n",
      "\tBefore: 1 categorical cols\n",
      "\tAfter: 14\n",
      "Dataset:card\n",
      "\tBefore: 21 numeric cols\n",
      "\tAfter: 83\n",
      "Dataset:loan\n",
      "\tBefore: 98 numeric cols\n",
      "\tAfter: 376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48744, 1367)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=full_df(path_dict, True, mode='test')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(train.columns)-set(test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f90f7c",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db983a",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53ed598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_filter(df, thresh, corr_path, mode='train'):\n",
    "    if mode=='train':\n",
    "        corr_mat=df.drop(['TARGET'], axis=1).corr().abs()\n",
    "        upper=corr_mat.where(np.triu(np.ones(corr_mat.shape), k=1).astype(np.bool))\n",
    "        upper.to_csv(corr_path, index=False)\n",
    "    elif mode=='test':\n",
    "        upper=pd.read_csv(corr_path)\n",
    "    to_drop=[column for column in upper.columns if any(upper[column]>thresh)]\n",
    "    print(f'Correlation: {len(to_drop)} will be removed')\n",
    "    return df.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae749e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akim.Alikhanov\\AppData\\Local\\Temp\\ipykernel_9372\\2888930088.py:4: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  upper=corr_mat.where(np.triu(np.ones(corr_mat.shape), k=1).astype(np.bool))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 619 will be removed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(307511, 749)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=correlation_filter(train, 0.9, '../data/corr_matrix.csv', mode='train')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ea72606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 619 will be removed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48744, 748)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=correlation_filter(test, 0.9, '../data/corr_matrix.csv', mode='test')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec93d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(train.columns)-set(test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55260cfa",
   "metadata": {},
   "source": [
    "## Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1ac8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_filter(df, thresh, col_path, mode='train'):\n",
    "    if mode=='train':\n",
    "        mt=miss_table(df)\n",
    "        to_drop=mt.loc[mt['Percent']>thresh].index\n",
    "        with open(col_path, 'wb') as f:\n",
    "            pickle.dump(to_drop, f)\n",
    "    elif mode=='test':\n",
    "        with open(col_path, 'rb') as f:\n",
    "            to_drop=pickle.load(f)\n",
    "    else:\n",
    "        raise Exception(\"Specify mode (train of test) and path\")\n",
    "    print(f'{len(to_drop)} features with {thresh}% of NaNs will be removed')\n",
    "    return df.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ee8f7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 689/749 columns with missing values\n",
      "Distribution by dtypes:\n",
      "float64    525\n",
      "float32    158\n",
      "object       6\n",
      "Name: Dtype, dtype: int64\n",
      "16 features with 80% of NaNs will be removed\n"
     ]
    }
   ],
   "source": [
    "train=missing_filter(train, 80, '../data/missing_columns_drop.pkl', 'train',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d32315f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 features with 80% of NaNs will be removed\n"
     ]
    }
   ],
   "source": [
    "test=missing_filter(test, 80, '../data/missing_columns_drop.pkl', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abe64680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TARGET'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train.columns)-set(test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29efce",
   "metadata": {},
   "source": [
    "## Zero Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d068116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_var_filter(df, col_path, mode='train'):\n",
    "    if mode=='train':\n",
    "        scaler=MinMaxScaler()\n",
    "        numeric_data=df.drop('SK_ID_CURR', axis=1).select_dtypes('number').reset_index(drop=True)\n",
    "        numeric_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        for i in numeric_data.columns:\n",
    "            numeric_data[i].fillna(value=numeric_data[i].mean(), inplace=True) #replace NaN with mean of dimension\n",
    "            numeric_data[i]=scaler.fit_transform(numeric_data[i].values.reshape(-1,1)) \n",
    "        vars_df=numeric_data.var()\n",
    "        to_drop=vars_df[vars_df==0].index\n",
    "        with open(col_path, 'wb') as f:\n",
    "            pickle.dump(to_drop, f)\n",
    "    elif mode=='test':\n",
    "        with open(col_path, 'rb') as f:\n",
    "            to_drop=pickle.load(f)\n",
    "    print(f'{len(to_drop)} features with zero variance will be removed')\n",
    "    return df.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06ce766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 features with zero variance will be removed\n"
     ]
    }
   ],
   "source": [
    "train=zero_var_filter(train, '../data/zero_var_columns_drop.pkl', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c44b0aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 features with zero variance will be removed\n"
     ]
    }
   ],
   "source": [
    "test=zero_var_filter(test, '../data/zero_var_columns_drop.pkl', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6302e29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TARGET'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train.columns)-set(test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a89184",
   "metadata": {},
   "source": [
    "## Zero Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73aba613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp_cv(X, y=None, k=5, params=None, mode='train'):\n",
    "    if mode=='train':\n",
    "        feat_importances_gain, feat_importances_split=[], []\n",
    "        kfold=StratifiedKFold(k)\n",
    "        for f, (tr, te) in enumerate(kfold.split(features, y=target)):\n",
    "            X_train, y_train=X.iloc[tr, :], y.iloc[tr]\n",
    "            X_test, y_test=X.iloc[te, :], y.iloc[te]\n",
    "            weight=np.count_nonzero(y_train==0)/np.count_nonzero(y_train==1)\n",
    "            dtrain=lgb.Dataset(X_train, label=y_train, params={'verbose': -1})\n",
    "            dval=lgb.Dataset(X_test, label=y_test, params={'verbose': -1})\n",
    "            model=lgb.train(\n",
    "                            params=params,\n",
    "                            train_set=dtrain,\n",
    "                            valid_sets=[dtrain, dval],\n",
    "                            valid_names=['train', 'test'],\n",
    "                            categorical_feature=list(X.select_dtypes('category').columns),\n",
    "                            callbacks=[lgb.early_stopping(100, verbose=-1)],\n",
    "                            verbose_eval=False)\n",
    "            feat_importances_gain.append(model.feature_importance(importance_type='gain'))\n",
    "            feat_importances_split.append(model.feature_importance(importance_type='split'))\n",
    "    elif mode=='test':\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
